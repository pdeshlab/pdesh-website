[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "data",
    "section": "",
    "text": "This page does not include replication data for specific papers. For these data, please go to Research. If you click on a paper of interest, you’ll find a button that will direct you to the replication data.\n\n\nDeshpande, Pia; Hartnett, Brendan; Schaffner, Brian; Ansolabehere, Steve, 2022, “CES 2020 BLM and COVID Data”, https://doi.org/10.7910/DVN/RLO7G9, Harvard Dataverse, V1\n\n\nSupplemental data files for the 2020 Cooperative Congressional Election Study. Datasets include questions on COVID-19 and questions on police brutality and Black Lives Matter. This project was supported by the National Science Foundation, Grant Number SES-1948863.\n\n\n\nsee data\n\n\n\nTo cite this data using BibTeX, use the following:\n\n\n@data{DVN/RLO7G9_2022,\nauthor = {Deshpande, Pia and Hartnett, Brendan and Schaffner, Brian and Ansolabehere, Steve},\npublisher = {Harvard Dataverse},\ntitle = {{CES 2020 BLM and COVID Data}},\nyear = {2022},\nversion = {V1},\ndoi = {10.7910/DVN/RLO7G9},\nurl = {https://doi.org/10.7910/DVN/RLO7G9}\n}\n\n\n\n\n\nDeshpande, Pia; Hershewe, Thomas, 2021,“CES Political Participation 2008-2022”, https://doi.org/10.7910/DVN/JUX8KA, Harvard Dataverse, V4\n\n\nThis dataset is a cumulative file of political participation questions in the CES from 2008 to 2022. The initial purpose of this dataset is for use in a tutorial to teach students, though it can be used for other purposes. Pia Deshpande initially created the dataset for years 2008 through 2020. Thomas Hershewe added the year 2022.\n\n\n\nsee data\n\n\n\nTo cite this data using BibTeX, use the following:\n\n\n@data{DVN/JUX8KA_2021,\nauthor = {Deshpande, Pia and Hershewe, Thomas},\npublisher = {Harvard Dataverse},\ntitle = {{CES Political Participation 2008-2022}},\nyear = {2021},\nversion = {V4},\ndoi = {10.7910/DVN/JUX8KA},\nurl = {https://doi.org/10.7910/DVN/JUX8KA}\n}\n\n\n\n\n\nDeshpande, Pia, 2022,“CCES 2020 Supplemental Data”, https://doi.org/10.7910/DVN/6NV9G3, Harvard Dataverse, V1\n\n\nSupplemental data files for the 2020 Cooperative Congressional Election Study. Datasets include the race and ethnicity of congressional candidates.\n\n\n\nsee data\n\n\n\nTo cite this data using BibTeX, use the following:\n\n\n@data{DVN/6NV9G3_2022,\nauthor = {Deshpande, Pia},\npublisher = {Harvard Dataverse},\ntitle = {{CCES 2020 Supplemental Data}},\nUNF = {UNF:6:gjKprISuP6j++pufPR7Ncg==},\nyear = {2022},\nversion = {V1},\ndoi = {10.7910/DVN/6NV9G3},\nurl = {https://doi.org/10.7910/DVN/6NV9G3}\n}\n\n\n\n\n\nDeshpande, Pia, 2022,“CCES 2018 Supplemental Data”, https://doi.org/10.7910/DVN/IA9SND, Harvard Dataverse, V4\n\n\nSupplemental data files for the 2018 Cooperative Congressional Election Study. Datasets include the race and ethnicity of congressional candidates.\n\n\n\nsee data\n\n\n\nTo cite this data using BibTeX, use the following:\n\n\ndata{DVN/IA9SND_2022,\nauthor = {Deshpande, Pia},\npublisher = {Harvard Dataverse},\ntitle = {{CCES 2018 Supplemental Data}},\nUNF = {UNF:6:s/hg03MO7J1RkXiipXY0MQ==},\nyear = {2022},\nversion = {V4},\ndoi = {10.7910/DVN/IA9SND},\nurl = {https://doi.org/10.7910/DVN/IA9SND}\n}",
    "crumbs": [
      "data"
    ]
  },
  {
    "objectID": "tutorial1.html",
    "href": "tutorial1.html",
    "title": "An introduction to functions in R",
    "section": "",
    "text": "Published: December 13, 2021",
    "crumbs": [
      "tutorials",
      "Introduction to functions in R"
    ]
  },
  {
    "objectID": "tutorial1.html#what-youll-learn",
    "href": "tutorial1.html#what-youll-learn",
    "title": "An introduction to functions in R",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nWhat functions in R do, and how to write some of them.\nWhen to use functions!\n\nI am not an expert on functions, and have been lucky to learn from well-written online resources. Hadley Wickham’s chapter on functions in Advanced R provides a helpful extension to this tutorial for those interested in writing more complex and useful functions. Wickham and Garrett Grolemund also write a chapter on functions in their introductory text, R for Data Science. Ken Rice and Thomas Lumley have an approachable slide deck that also includes an intro to Shiny in R — a useful way to display interactive outputs (a tutorial on Shiny is coming soon!).",
    "crumbs": [
      "tutorials",
      "Introduction to functions in R"
    ]
  },
  {
    "objectID": "tutorial1.html#whats-a-function",
    "href": "tutorial1.html#whats-a-function",
    "title": "An introduction to functions in R",
    "section": "What’s a function?",
    "text": "What’s a function?\nYou use functions every single time you code, whether they are in base R, or a package you load in. For example, every time you have used the dplyr library to mutate a new column in a dataset, you are calling on a function. Lucky for you, the kind folks who created dplyr have already coded out what exactly mutate does, so all you have to do is call the function. Yet, there are some times when the right function just isn’t out there, and you’ll need to write one yourself. How would you even start?\nWell, let’s start with the basic syntax of a function. Functions need a name, arguments, and a body. The name of the function is what term you will use to call it later. Arguments are values, dataframes, or other entities you feed into functions to be worked upon. The body of a function is a set of instructions telling R exactly what to do to your arguments.\nSeems complicated? It can be! But it can also be really simple. See below:\n\n# We first have to name our function.\n# I named mine \"multiply\" Try to \n# pick an informative name that you'll\n# remember.\nmultiply &lt;- function(argument1, argument2)\n              # We then need to tell our\n              # function what arguments we\n              # pass through it!\n  {argument1*argument2}\n   # what happens to the arguments\n\n# we are calling the function here\n# and setting argument1 = 1 and \n# argument2 =2. \nmultiply(1,2)\n\n[1] 2\n\n\n\n# I could have also written:\n# multiply(argument1 = 1, argument2 = 2)\n\nA function can be as easy as argument1*argument2! You can even nest functions within each other:\n\nadd &lt;- function(number1, number2)\n  {number1 + number2}\n\nadd(multiply(1,2), 4)\n\n[1] 6\n\n\n\n# What's going on here?\n# number1 = multiply(1,2)\n# Try to do the math manually\n# to verify that the answer makes\n# sense.\n\nMore complex functions can use if else statements, for loops, and a host of other fun R quirks! These function types can differ from the syntax we define above.\n\nIf else functions\nIf else functions can take on a different syntax than our standard function. See below:\n\nx = 7\n\nifelse(x &gt; 6, TRUE, FALSE)\n\n[1] TRUE\n\n\nWe read the above like this: “If x is greater than 6, return TRUE. Else, return FALSE.” Because we have defined x = 7, we get TRUE in this case! This function also works if x is a list of many numbers, and will return TRUE and FALSE values for each number included in x — though the output is messy:\n\nx &lt;- c(-5:19)\n# Let x be a vector ranging from -5 to 19.\n# Feel free to print x to get a closer look.\n\nifelse(x &gt; 6, TRUE, FALSE)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[25]  TRUE\n\n\n\n\nFor loops\nLet’s say I’m trying to brainstorm potential restaurants to bring a date and I need to decide on a type of food. Now, this is a nonsensical way for anyone but a social scientist to do this (just ask your date what they like?). I’ve made a list, called food that contains types of food I’ve been craving.\n\nfood &lt;- c(\"Indian\", \n          \"Chinese\", \n          \"Japanese\", \n          \"Laotian\", \n          \"Italian\", \n          \"Mexican\")\n\n# For each element in list food, print!\nfor(x in food){\n  print(x)\n}\n\n[1] \"Indian\"\n[1] \"Chinese\"\n[1] \"Japanese\"\n[1] \"Laotian\"\n[1] \"Italian\"\n[1] \"Mexican\"\n\n\nMy little function prints my list again! Let’s see how we can adapt our function to be more useful. Assume I took my previous advice and decided to communicate with my date. They gave me a list of food they’ve been craving, and I’ve written it down in a list called food_beloved. How do I figure out what types of food we both like? Well. . . we could write a function! We’ll need to use an if statement.\n\nfood &lt;- c(\"Indian\", \n          \"Chinese\", \n          \"Japanese\", \n          \"Laotian\", \n          \"Italian\", \n          \"Mexican\")\n\nfood_beloved &lt;- c(\"Laotian\", \n                  \"Chinese\", \n                  \"French\", \n                  \"Ethiopian\")\n\n# For each element in list food ---\n# IF the element is also in food_beloved,\n# print it!\nfor(x in food){\n  if (x %in% food_beloved){\n    print(x)\n  }\n}\n\n[1] \"Chinese\"\n[1] \"Laotian\"\n\n\nAlright! Chinese or Laotian it is then. Dating aside, this type of function can be really useful when you need to iterate through a list or a column of a dataframe and do the same operation to each element. As you can see, that operation can be a type of filtering (like above), but it could also be a data transformation or changing a data type! A lot of functions written by extremely smart people already do this for us (take the case_when function, for example), but it’s still good to know how to write them in case you’re doing some really newfangled stuff.\nNow, I suppose I should mention that for loops have a terrible PR team. They get bad press because coders believe they are less efficient than functions that use lapply, though this is only partially true (see Hadley Wickham’s post for more details). Let’s look at some example lapply functions now.\n\n\nFunctions using lapply\nI’ve missed my date and am trying to rewrite my function using lapply. How would I do it? Well first, I would need to write a function the old-fashioned way — using the syntax we first established at the beginning of this tutorial. Then, I’ll need to let lapply apply that function to all elements of a list (like food!).\nSee if you can tell what’s going on below!\n\nfirstdate &lt;- function(x){\n  if(x %in% food_beloved){\n    print(x)\n  }\n}\n\ninvisible(lapply(food, firstdate))\n\n[1] \"Chinese\"\n[1] \"Laotian\"\n\n\n\n# I use invisible to suppress lapply\n# printing the output you see \n# and additional output, but sometimes you \n# want all that information!\n# Try running this code without invisible(), and see\n# what output you get!\n\nOkay— new problem. You wrote a test but really messed up the wording of one question (Tocqueville is hard to spell, and you managed to beef it so badly no student recognized his name on the exam). You’re not cruel or unusual so you want to give students back points for this question. It was worth 5 points, and you have all your students’ grades in a handy list.\n\n# You should really get them to come to office hours\ngrades &lt;- c(50, 90, 85, 95, 71, 83, 90)\n\n\nboost &lt;- function(x){\n  \n  # Give everyone 5 points back please!\n  x + 5\n  }\n\n# Please apply the boost function\n# to each element of grades\nlapply(grades, boost)\n\n[[1]]\n[1] 55\n\n[[2]]\n[1] 95\n\n[[3]]\n[1] 90\n\n[[4]]\n[1] 100\n\n[[5]]\n[1] 76\n\n[[6]]\n[1] 88\n\n[[7]]\n[1] 95\n\n\nNow, an astute reader may see some problems with this function. If a student got the bogus question correct by chance, they shouldn’t receive an additional five points, no matter how terrible your test-writing abilities are. You can specify all these limitations with if else statements. It is also worth nothing that if your grades come in data frame form, you could easily use dplyr to fix this problem!\nNow that you know a bit about functions, what are the reasons for and against using them while you code?",
    "crumbs": [
      "tutorials",
      "Introduction to functions in R"
    ]
  },
  {
    "objectID": "tutorial1.html#reasons-you-should-use-functions",
    "href": "tutorial1.html#reasons-you-should-use-functions",
    "title": "An introduction to functions in R",
    "section": "Reasons you should use functions",
    "text": "Reasons you should use functions\n\nIt can make code less tedious to write.\nThis is dependent upon your coding and/or drafting style. If tedium makes you unwilling to code, then it may make sense to sit and think about functions before brute forcing a problem.\n\n\nIt makes your code more intelligible because there’s less of it.\nHave you ever looked back at old code only to realize it’s 1000 lines long and you didn’t annotate anything and oh my god how are you going to make this work? Yeah. Functions can help a bit with that! Nothing is a substitute for good documentation, but the longer your code is, often the harder it is to interpret. If a function can shorten your code without making it harder to understand, you should consider writing some! It will also make re-reading your code less painful.\n\n\nYou need to write one to accomplish your data wrangling task.\nThere are some tasks that just can’t reasonably be accomplished without a function! This is why smart people write R packages to help us (thank you dplyr). But sometimes, even packages written by others are not enough — you need to write your own code. What if you need to load in a bunch of data saved on your computer stored in separate CSV files. Luckily for you, the data is named in an organized way that that is patterned (“data1.csv,” “data2.csv,” you get the picture). You don’t want to laboriously download each CSV one by one (that’s a line of code for each CSV!). Sometimes you can brute force a solution, but if you have enough literal or metaphorical CSVs, you’ll need to write a function instead. If you think a task will take you many hours of mindless typing, stop and consider writing a function instead! We use code to make our lives easier and less repetitive, so unless you want to go back to running regressions by hand, try and take the lazier way out — write a function!",
    "crumbs": [
      "tutorials",
      "Introduction to functions in R"
    ]
  },
  {
    "objectID": "tutorial1.html#reasons-you-may-not-want-to-use-functions",
    "href": "tutorial1.html#reasons-you-may-not-want-to-use-functions",
    "title": "An introduction to functions in R",
    "section": "Reasons you may not want to use functions",
    "text": "Reasons you may not want to use functions\n\nYou’re drafting and it slows down your coding flow.\nI’m of the strong opinion that everyone codes differently. If you are collaborating with someone, you may want to talk about best group coding practices, but it is okay if your style of coding differs from your peers. When I first write my code, I rarely use functions. Functions are added upon review or if I need to accomplish a task that would be incredibly tedious without a handy function. I think this is because of my writing and coding style! When I write (or code), I want to get a first draft out as soon as possible. After that, I can go back and edit for efficiency, clarity, and style. You may code differently and find it helpful to go slowly and be as meticulous as possible. That’s also totally fine! But tl;dr: if writing functions is making your workflow slow, save function writing for the editing phase!\n\n\nIt makes your code harder to understand, even with annotations.\nSometimes, consolidating code with functions can make the code difficult to read, especially if the functions are complex. Many R users will be familiar with what the mutate function does, but they won’t be familiar with a function you wrote! Make sure to clearly document your functions and use annotations to explain how you’re writing them and why you are using them. I prize code intelligibility above most other things, but your mileage may vary.\n\n\nThe functions you’re writing are only being used once or twice, and don’t shorten the code all that much.\nFunctions should make your code easier to write, read, and understand. If you write a function only to use it once or twice, you might want to reconsider. Now, there are functions designed to only be used once (for example, take a function that loads a bunch of data based on a file name pattern). That is absolutely okay if it makes your life noticeably easier! However, writing a data wrangling function and only using it once usually defeats the point of writing the function!",
    "crumbs": [
      "tutorials",
      "Introduction to functions in R"
    ]
  },
  {
    "objectID": "tutorial1.html#an-example-lets-consolidate-our-line-chart-code",
    "href": "tutorial1.html#an-example-lets-consolidate-our-line-chart-code",
    "title": "An introduction to functions in R",
    "section": "An example: let’s consolidate our line chart code",
    "text": "An example: let’s consolidate our line chart code\nWe will be adapting code from my first tutorial, Plotting trends over time with the CES. Make sure you have the code from this tutorial at the ready (the entire script can be found at the bottom of that tutorial page).\nIf you look at the code from the tutorial in question, we repeat a lot of the same analysis on different subsets of data. For example, we use the survey package to analyze Democrats and Republicans’ attendance of political meetings, party donations, and putting up campaign signs. See this redundant code below:\n\n# pol_meet By Party\npol_meet &lt;- as.data.frame(svyby(~pol_meet_recode, \n                                ~year, \n                                survey, \n                                svymean, \n                                na.rm = TRUE))\n\npol_meet_rep &lt;- as.data.frame(svyby(~pol_meet_recode, \n                                    ~year, \n                                    subset(survey, pid3 == 2),\n                                    svymean, \n                                    na.rm = T)) %&gt;%\n  mutate(party = \"Republican\")\n\npol_meet_dem &lt;- as.data.frame(svyby(~pol_meet_recode, \n                                    ~year, \n                                    subset(survey, pid3 == 1), \n                                    svymean, \n                                    na.rm = T)) %&gt;%\n  mutate(party = \"Democrat\")\n\n\npol_meet_party &lt;- pol_meet_rep %&gt;% \n  bind_rows(pol_meet_dem)\n\n\n# donate_candidate by Party\ndonate_candidate_rep &lt;- as.data.frame(\n  svyby(~donate_candidate_recode, \n        ~year, \n        subset(survey, pid3 == 2), \n        svymean, na.rm = T)) %&gt;%\n  mutate(party = \"Republican\")\n\ndonate_candidate_dem &lt;- as.data.frame(\n  svyby(~donate_candidate_recode, \n        ~year, \n        subset(survey, pid3 == 1), \n        svymean, \n        na.rm = T)) %&gt;%\n  mutate(party = \"Democrat\")\n\n\ndonate_candidate_party &lt;- donate_candidate_rep %&gt;% \n  bind_rows(donate_candidate_dem) \n\n\n# These nearly identical code chunks repeat \n# for every variable we're interested in analyzing \n# by party. Surely there's an easier way!\n\nHow might we consolidate this code? Luckily, writing a function will be quite easy once we see the pattern in our script. We’ve already done the hard work!\n\n# All of this code uses work we've\n# already done in the previous coding chunk\nparty_analysis &lt;- function(measure){\n\nrep &lt;- as.data.frame(\n  svyby(~measure, \n        ~year, \n        subset(survey, pid3 == 2), \n        FUN = svymean, \n        keep.names = F,\n        na.rm = T)) %&gt;%\n  mutate(party = \"Republican\")\n\ndem &lt;- as.data.frame(\n  svyby(~measure, \n        ~year, \n        subset(survey, pid3 == 1), \n        FUN = svymean, \n        keep.names = F,\n        na.rm = T)) %&gt;%\n  mutate(party = \"Democrat\")\n\n\nparty &lt;- rep %&gt;% \n  bind_rows(dem)\n\n# Hey R! Return party when you're done with\n# this function.\nreturn(party)\n}\n\n\nparty_analysis(pol_meet_recode)\n\nUh. . .okay. Let’s try this a different way?\n\nparty_analysis(ces_participation$pol_meet_recode)\n\nOh no! Not only did our function break— it broke in two distinct ways when we tried to fix it! The first error we got when running party_analysis(pol_meet_recode) was one that made some sense. pol_meet_recode isn’t an object in our R environment. It’s a column in ces_participation and part of a svyobject we created earlier in our code. However, when we try to call it by linking it to its parent data frame (ces_participation$pol_meet_recode), we get this super weird error about subscripts. What’s going on?\nReader, I did not know what was going on. But I found out! I learned for you! It turns out that functions can sometimes be really picky. Any function you write that uses a data frame column name as an argument is likely to break unless you use special notation (read more on this by Bryan Shalloway). This is because R functions need to know where to look for their arguments. They default to the R environment. . . and columns just aren’t there!\nBut it turns out Shalloway’s expert advice on functions using column names won’t help us here. At least, it won’t fix the whole thing. Our obstacle has to do with the survey package, and any other packages that use formulas. The package takes arguments formatted in a very specific way ~variable, which proves troubling for us. To get around it, we need to specify that the function takes a character string as.formula(), and use the paste function to prefix the relevant variable with a ~. The eval and get functions are a bit more complicated. Essentially, I needed to tell R to get the character string I give it, evaluate it from the environment, and then slap a good ole tilde right in front of it before my function can run.\nIf any coding wizards see this tutorial and can think of a better way to do this, please let me know. I will include it in this tutorial and it will likely save everyone a big headache.\nUntil a genius comes by, the below workaround works well! Special thanks to this Stack Overflow answer by Ter for the coding approach. Without them, we would all be lost. (Also, Ter both asked and answered his own Stack Overflow question — which I feel deserves some kind of Nobel Prize).\n\nlibrary(dplyr)\nlibrary(survey)\nparty_analysis &lt;- function(measure){\n\nrep &lt;- as.data.frame(\n  svyby(\n    as.formula(\n      paste(\"~\" , eval(get(\"measure\")))), \n                           ~year, \n                          subset(survey, pid3 == 2), \n                          FUN = svymean, \n                          keep.names = F,\n                          na.rm = T)) %&gt;%\n  mutate(party = \"Republican\")\n\ndem &lt;- as.data.frame(\n  svyby(\n    as.formula(\n      paste(\"~\" , eval(get(\"measure\")))), \n                           ~year, \n                           subset(survey, pid3 == 1), \n                           FUN = svymean, \n                           keep.names = F,\n                           na.rm = T)) %&gt;%\n  mutate(party = \"Democrat\")\n\n\nparty &lt;- rep %&gt;% \n  bind_rows(dem)\n\nreturn(party)\n}\n\nIf this function actually works, we should be able to use it on any political participation measure, and really consolidate our code. Sounds excellent!\n\npol_meet_party_2 &lt;- party_analysis(measure = \"pol_meet_recode\")\npol_meet_party_2\n\n   year pol_meet_recode          se      party\n1  2008      0.14114705 0.004002460 Republican\n2  2010      0.17256445 0.004274270 Republican\n3  2012      0.12910268 0.004079811 Republican\n4  2014      0.11547733 0.003905133 Republican\n5  2016      0.10185321 0.003571707 Republican\n6  2018      0.11007146 0.003635246 Republican\n7  2020      0.07620658 0.003123227 Republican\n8  2022      0.08838090 0.003295866 Republican\n9  2008      0.14488431 0.004163189   Democrat\n10 2010      0.13199639 0.003515532   Democrat\n11 2012      0.11290912 0.003488990   Democrat\n12 2014      0.12065632 0.003403809   Democrat\n13 2016      0.12543229 0.003713777   Democrat\n14 2018      0.14307637 0.003720744   Democrat\n15 2020      0.08235351 0.002771559   Democrat\n16 2022      0.09990017 0.003014817   Democrat\n\n\nHooray! Our function actually works. But how do we know it works as intended? There are many ways to test your functions, and the way you test them is dependent on what the function is meant to do. Actually, there is a whole package devoted to testing your function and your code, which you can read more about here.\nWe won’t use this package for this tutorial. Luckily, we know what the function is supposed to do, and we have a version of the code that does not use a function. pol_meet_party_2 should be the exact same data frame as pol_meet_party, but how do we verify that? We can use anti_join, which checks to see what does not match between two data frames. Just like other joins, anti_join works by matching on certain columns. If you are trying to check if two data frames are identical, the function should be matching on all columns, which it will list for you in your console.\n\nanti_join(pol_meet_party_2, pol_meet_party)\n\nJoining with `by = join_by(year, pol_meet_recode, se, party)`\n\n\n[1] year            pol_meet_recode se              party          \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n# anti-join returns no results! looks like\n# we have two identical data frames!\n\nThat brings us to the end of our introductory tour of functions! This is a sprawling topic, and you’ll likely learn more about functions as new and terrifying data problems arise when you work on projects. And remember, if you write a host of functions you think will be useful to the coding community, you can publish them in a package yourself! Think how much more dismal coding would be without your favorite packages — you could really make somebody’s day!",
    "crumbs": [
      "tutorials",
      "Introduction to functions in R"
    ]
  },
  {
    "objectID": "tutorial3.html",
    "href": "tutorial3.html",
    "title": "Making maps with the CES",
    "section": "",
    "text": "Published: November 29, 2021",
    "crumbs": [
      "tutorials",
      "Making maps with the CES"
    ]
  },
  {
    "objectID": "tutorial3.html#what-youll-learn",
    "href": "tutorial3.html#what-youll-learn",
    "title": "Making maps with the CES",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nHow to use parts of the urbnmapr package to make some maps\nUsing ggplot2 attributes to make your maps more readable\nWhen to not use maps (or at least think carefully before doing so)\n\nWhat won’t you learn? There is a lot that goes into maps: projection types (see Dan Kelley’s excellent writeup on the subject), zip code fuzziness, and loading in shapefiles from the Census, just to name a few topics. Cartographers are beings of infinite knowledge and power, and I won’t be able to teach you all of their secrets (I don’t know most of them!). But don’t let that dissuade you! If you’re passionate about maps, this will be a good place for you to start. And if you just need to plot a state-level election outcome map for a class and are panicking, perhaps this tutorial will help you too.\nWe’ll also be using the dataverse and survey packages in this tutorial. I will not explain them in detail here. I have discussed them (and provided additional resources) in my last post: Plotting trends over time with the CES.\nMuch of this tutorial uses code that is adapted from an article by Christopher Goodman and the Urban Institute’s very own tutorial. When I do this adapting, I note it in-text, but I wanted to give credit up-top too. Thank you for these incredible resources!\nAs always, we need to start our code by loading in packages. You’ll need the following to packages execute my code, which you can install with install.packages(\"name\"). You’ll then load them in like so:\n\n#### LOAD PACKAGES ####\nlibrary(dataverse) # loading data\nlibrary(urbnmapr)  # geographic information for mapping!\nlibrary(ggplot2)   # pretty plots\nlibrary(geofacet)  # tile plots\nlibrary(scales)    # easy formatting\nlibrary(dplyr)     # data manipulation\nlibrary(survey)    # survey analysis\nlibrary(tidyverse) # the holy grail: more data wrangling\n\nlibrary(leaflet)   # optional: for interactive maps!\nlibrary(here)\n\nThe only exception to this is the urbnmapr package, which you need to install from GitHub. This is easy to do! Make sure you have the devtools installed as well and run what is below. If you are using a Mac, you may run into some problems loading the geofacet package. If this happens, follow this guidance on Stack Overflow, but apply it to the packages that did not load correctly. You may need to remove package installations and re-install them. In general, make sure you have the sf package installed prior to attempting to re-install geofacet.\n\nrequire(\"devtools\")\ndevtools::install_github(\"UrbanInstitute/urbnmapr\")",
    "crumbs": [
      "tutorials",
      "Making maps with the CES"
    ]
  },
  {
    "objectID": "tutorial3.html#getting-the-data",
    "href": "tutorial3.html#getting-the-data",
    "title": "Making maps with the CES",
    "section": "Getting the data",
    "text": "Getting the data\nWe’ll load in the 2020 CES data using the dataverse package.\n\n##### LOADING DATA ####\nces2020_dataverse &lt;- get_dataframe_by_name(\n  filename = \"CES20_Common_OUTPUT_vv.dta\",\n  dataset = \"10.7910/DVN/E9N6PH\",\n  original = TRUE,\n  .f = haven::read_dta,\n  server = \"dataverse.harvard.edu\"\n)\n\nNow before proceeding, we’ll need to decide what questions we want to answer with maps. Picking these questions is important, and not something this tutorial can completely teach you. I’ll include a checklist that I like to use when I have an urge to make a map, but you’ll have to do some thinking on your own, too.\nI’m interested in something methodological. The CES—and other surveys like it— often have multiple waves. There are many reasons for doing this. Maybe you want to see how respondents’ opinions will change after an important event (an election? The release of Red (Taylor’s Version)?). Perhaps you want to interview the same respondents again and again (called a panel study). Whatever your reason, you now have one main obstacle: attrition. It’s hard to get people to take surveys. Now getting those same people to take another one? It’s a tall task! I want to calculate what percentage of respondents failed to take the CES’ post-election survey in 2020 by state. And I want to map it!\nLet’s start by selecting relevant questions. We’ll need some kind of case identifier, the variable of interest, and a geography variable of some kind (to use in mapping). I am most interested in state differences, so I am selecting the inputstate variable.\nI do some other data manipulation in the following code to make things run smoothly (made sure state FIPS codes are interpreted correctly, and recoded the tookpost variable). I’ll explain what I’m doing in more detail in the code chunk!\n\n# Let's select variables that we care about!\nces2020_selected &lt;- ces2020_dataverse %&gt;%\n  select(\n    \n    # id for each respondent\n    caseid, \n    \n    # did you take the post-election survey?\n    tookpost, \n    \n    # state respondent is living in.\n    # In this code, I am both selecting\n    # the inputstate column and renaming\n    # it to state_fips rough, all in\n    # one line!\n    state_fips_rough = inputstate) %&gt;%\n  mutate(\n    \n    # tookpost is originally coded so 2 = Yes\n    # and 1 = No. I am recoding it so Yes = 1\n    # and No = 0. Why? When I do calculations, \n    # the average of tookpost will be a proportion\n    # between 0 and 1 that I can represent as a \n    # percent (ex. \"85% of respondents took the\n    # post-election survey\").\n    tookpost_recoded = case_when(\n      tookpost == 1 ~ 0,\n      tookpost == 2 ~ 1,\n    ),\n    \n    # why am I changing state_fips_rough? Like\n    # most coding, things were breaking which inspired\n    # me to get creative. Try running this chunk without\n    # this line and look at ces2020_selected. See anything\n    # weird? Check before you read on!\n    state_fips = as.character(sprintf(\"%02d\", state_fips_rough))\n    \n    # Okay, now that you've looked, I'll spill the beans. \n    # State FIPS codes are always two digits, like 15, 50, \n    # and 02. R was reading in single digits like 02 as\n    # just 2, which is a problem later on. To fix this\n    # I used the function sprintf to tell R to add\n    # leading zeroes to state_fips_rough until there\n    # were two digits per entry. So a value of 4 would\n    # turn into 04, but 36 would stay 36! I can't spend\n    # more time on this now, but if people are interested\n    # in this kind of data manipulation, I am happy to write\n    # about it!\n  )\n\nBefore we move forward, I want to see what sample sizes we’re working with. I can do this with the lovely dplyr and using piping.\n\n# hey R, for the following, use ces2020_selected please!\nces2020_selected %&gt;%\n  \n  # whatever calculation I tell you to do, please\n  # do it grouped--- that is, only within entries\n  # with the same state FIPS code.\n  group_by(state_fips) %&gt;%\n  \n  # could you tell me how many entries I have per\n  # state?\n  summarise(n = n())\n\n# A tibble: 51 × 2\n   state_fips     n\n   &lt;chr&gt;      &lt;int&gt;\n 1 01           947\n 2 02           115\n 3 04          1463\n 4 05           536\n 5 06          5035\n 6 08          1061\n 7 09           642\n 8 10           240\n 9 11           197\n10 12          4615\n# ℹ 41 more rows\n\n\n\n# you'll notice that the whole table does not print here.\n# To see the entire thing, print it in your R console\n\nInteresting! It’s cool to see how the CES samples across the United States. Because I’m interested in just looking at the CES as is (and not using the CES to make a hypothesis about Americans writ large), I don’t need to do any other steps here. These numbers will be good to keep in mind as we calculate what percentage of respondents took the post-election survey by state. Actually, let’s go ahead and calculate that with dplyr!\n\npost_aggregate &lt;- ces2020_selected %&gt;%\n  group_by(state_fips) %&gt;%\n  \n  # give me the mean of tookpost_recoded \n  # for each state, and make that a new\n  # variable called mean_post\n  summarise(mean_post = mean(tookpost_recoded))\n\n# let's take a look!\npost_aggregate\n\n# A tibble: 51 × 2\n   state_fips mean_post\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 01             0.796\n 2 02             0.774\n 3 04             0.881\n 4 05             0.772\n 5 06             0.842\n 6 08             0.866\n 7 09             0.864\n 8 10             0.854\n 9 11             0.792\n10 12             0.851\n# ℹ 41 more rows\n\n\nNow it’s time to use the urbnmapr package developed by The Urban Institute. Mapping is actually quite complicated! Even though we now have a summary table that tells us what percentage of respondents took the post-election survey for each state, R still needs to be able to draw the map! How will it know what states are shaped like? How boundaries intersect? How will it make the map pretty?\nYou try drawing a map of the U.S. from scratch without guidlines. It’s hard! urbnmapr helps by letting us pull from its database. Below, I use our state_fips column to pull geographic details from urbnmapr. I can do this because urbnmapr has a databse with a column named state_fips (almost like I named it the same on purpose!).\n\npost_map &lt;- left_join(post_aggregate, \n                      urbnmapr::states, by = \"state_fips\")\n\npost_map\n\n# A tibble: 83,933 × 10\n   state_fips mean_post  long   lat order hole  piece group state_abbv\n   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt;     \n 1 01             0.796 -88.5  31.9     1 FALSE 1     01.1  AL        \n 2 01             0.796 -88.5  31.9     2 FALSE 1     01.1  AL        \n 3 01             0.796 -88.5  31.9     3 FALSE 1     01.1  AL        \n 4 01             0.796 -88.5  32.0     4 FALSE 1     01.1  AL        \n 5 01             0.796 -88.5  32.0     5 FALSE 1     01.1  AL        \n 6 01             0.796 -88.5  32.1     6 FALSE 1     01.1  AL        \n 7 01             0.796 -88.4  32.2     7 FALSE 1     01.1  AL        \n 8 01             0.796 -88.4  32.2     8 FALSE 1     01.1  AL        \n 9 01             0.796 -88.4  32.2     9 FALSE 1     01.1  AL        \n10 01             0.796 -88.4  32.3    10 FALSE 1     01.1  AL        \n# ℹ 83,923 more rows\n# ℹ 1 more variable: state_name &lt;chr&gt;\n\n\nWhen we print post_map, you’ll notice a lot of information that we didn’t have before. It looks like we have longitude and latitude, some other visual helpers for mapping, as well as helpful state abbreviations! We retain our other data (most importantly— our mean_post stat).\nBelieve it or not, it’s now time to map. The code for creating maps can be daunting, and I’ll present a big chunk to you with annotations. You should feel free to try this code with lines removed to see how the map would appear differently!\n\n# Plotting \n# Code adapted from: \n# https://www.cgoodman.com/blog/archives/2018/06/16/maps-in-r-using-urbnmapr/\n# I also retain some of Chris' annotations here for clarity.\n\nggplot() +\n  \n  # county map\n  geom_polygon(data = post_map,\n               mapping = aes(x = long, y = lat,\n                             group = group,\n                             \n                             # I want the color \n                             # that each state\n                             # has to depend\n                             # on mean_post!\n                             fill = mean_post)) +\n \n   # add state outlines using urbnmapr\n  geom_polygon(data = urbnmapr::states,\n               # when we write urbnmapr::states\n               # we are telling R to specifically\n               # use the urbnmapr package to pull up\n               # states. Some coders like using this notation\n               # consistently ---whenever they use a function\n               # they tell R which package it comes from.\n               # I don't do that, but if your code is\n               # breaking and you don't know why--- try\n               # calling on the package explicitly (sometimes)\n               # packages have functions that are named the\n               # same thing, which confuses R!\n               \n               mapping = aes(long, lat,group = group),\n               fill = NA, color = \"#ffffff\", size = 0.4) +\n  \n  # projection\n  coord_map(projection = \"polyconic\")+\n  \n  # I want to create a scale that goes from low to\n  # high, where the color of the high value connotes\n  # that things are good. In the United States,\n  # the color green is often the \"good\" or \"everything's\n  # fine\" color, so I'll use a version of that.\n  \n  # You can find an R Color Cheat Sheet here by Melanie Frazier: \n  # https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf\n  scale_fill_gradient(low = \"white\", \n                      high = \"seagreen4\", \n                      \n                      # I can define limits explicitly if I want.\n                      # If you do this, always check that you are not\n                      # accidentally excluding data! (I made sure all my)\n                      # data fit in this range reasonably.\n                      limits = c(0.75, 0.95), \n                      \n                      # Right now my code is in decimals.\n                      # We can use a trick we learned last tutorial\n                      # to make the labels into integer percentages.\n                      # Try changing the accuracy = 5L argument\n                      # to learn what it does!\n                      labels = percent_format(accuracy = 5L)) + \n  # Theming\n  theme_minimal()+\n  theme(\n    legend.position = \"right\",\n    legend.text.align = 0,\n    legend.title = element_blank(),\n    plot.margin = unit(c(.5,.5,.2,.5), \"cm\")) +\n    theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n  ) +\n  \n  # some useful titles!\n  labs(x = \"\",\n       y = \"\",\n       title = \"% of Respondents Who Took the Post Election Survey\",\n       caption = \"Author: Pia Deshpande, \\n Data: 2020 Cooperative Election Study\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nAnd ta da! A map! It seems like respondents don’t take the post election survey as frequenty in Mississippi, Arkansas, Oklahoma, Alaska, and Texas, among others. On the other hand, New Mexico and New Hampshire seem to be avid survey takers.\nThere are some good things about the map, and some bad things.\nLet’s start with the bad. All maps take data that is detailed, and represents that data in a less detailed way to be intelligible. I’m not sure exactly what percentage of Texans took the post-election survey, and to find out, I would need to pull up that table we created earlier! The color scale I picked gives me some idea how states are doing compared to each other, but not how they are doing compared to some norm. What if I wanted to see which states were below the CES’ national average? This would not be the plot to use then! Another bad thing is endemic to this type of map — I’ve talked about Texas twice now. Why? Well, it’s big on the map, and my eye wants to pay attention to it. The north east is dwarfed completely, and it makes it hard to see trends there.\nOkay, but are there good things? Yes! Using state geography has its benefits. Your readers will likely be familiar with this type of map. They’ll know where to glance for their home state, and will (hopefully) be able to locate others. My color scale progresses from white to green, which gives the effect of states with a lower response rate being kinda transparent. That’s neat! Conceptually, it makes sense that Oklahoma is fading out compared to New Mexico, which took the post-election survey at a much higher rate.\nWe shouldn’t just settle for this map as is! Though some problems can’t be solved with maps at all (not being able to see the data exactly) or with chloropleths (state geographies); some can be solved! Let’s make a map that will help me tell the world which states are doing above, below, and around average on taking the post-election survey. I’ll do this by changing the color scale.\n\n# Plotting (Code adapted from https://www.cgoodman.com/blog/archives/2018/06/16/maps-in-r-using-urbnmapr/)\n\nggplot() +\n  # County map\n  geom_polygon(data = post_map,\n               mapping = aes(x = long, y = lat,\n                             group = group,\n                             fill = mean_post)) +\n  # Add state outlines\n  geom_polygon(data = urbnmapr::states,\n               mapping = aes(long, lat,group = group),\n               fill = NA, color = \"#ffffff\", size = 0.4) +\n  # Projection\n  coord_map(projection = \"polyconic\")+\n  \n# scale_fill_gradient 2 helps us create *diverging* color scales!\n# I define red as a low point, white as the middle, and green\n# as the high point. \nscale_fill_gradient2(\n  low = \"red\",\n  mid = \"white\",\n  high = \"seagreen4\",\n  \n  # I define the midpoint explicitly here! \n  # I calculate it not as the mean of all the \n  # state averages, but as the mean of all CES\n  # respondents. That value will be where the\n  # scale is a stark white.\n  midpoint = mean(ces2020_selected$tookpost_recoded),\n  limits = c(0.75, 0.95), \n  labels = percent_format(accuracy = 5L)\n)+\n  # Theming\n  theme_minimal()+\n  theme(\n    legend.position = \"right\",\n    legend.text.align = 0,\n    legend.title = element_blank(),\n    plot.margin = unit(c(.5,.5,.2,.5), \"cm\")) +\n    theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n  ) + \n  labs(x = \"\",\n       y = \"\",\n       title = \"% of Respondents Who Took the Post Election Survey\",\n       caption = \"Author: Pia Deshpande, \\n Data: 2020 Cooperative Election Study\")\n\n\n\n\n\n\n\n\nWell this looks different! States that are around average fade into the background (goodbye Oregon!), while outliers are bleeding and verdant. Mississippi really stands out here, and so does New Hampshire!",
    "crumbs": [
      "tutorials",
      "Making maps with the CES"
    ]
  },
  {
    "objectID": "tutorial3.html#what-about-substantive-maps",
    "href": "tutorial3.html#what-about-substantive-maps",
    "title": "Making maps with the CES",
    "section": "What about substantive maps?",
    "text": "What about substantive maps?\nOkay, but I need to write a paper by the end of the semester using survey questions to show my teacher how weighting works. How do I make a map for that?\nHave no fear! This section will help you. Let’s map what percentage of Americans were contacted by a political campaign in 2020 and how that differs by state.\nMaking a substantive map means we need to be using all of our normal data analysis tools. That is, when we calculate summary statistics, we should use survey weights and the survey function. It also means being well aware of sample size. For this analysis, if a state has less than 200 observations, I exclude it from my analysis. Since that state will still be on the map, we’ll have to decide how we depict it.\nFirst, let’s select relevant variables from ces2020_dataverse, just like we did for the first set of maps!\n\n# Let's try making a substantive map: what percentage of respondents were contacted\n# by a political campaign in 2020?\n\n# Let's select variables that we care about!\nces2020_substantive &lt;- ces2020_dataverse %&gt;%\n  select(\n    \n    # id for each respondent\n    caseid, \n    \n    # weight for survey analysis!\n    commonpostweight, \n    \n    # state respondent is registered in\n    state_fips_rough = inputstate_post, \n    \n    # were you contacted by a political campaign in 2020?\n    CC20_431a) %&gt;%\n  \n  mutate(\n    contact = case_when(\n      \n      # I am recoding this so a \"Yes\" is a 1\n      # and a \"No\" is a 0.\n      CC20_431a == 1 ~ 1,\n      CC20_431a == 2 ~ 0\n    ),\n    state_fips = as.character(sprintf(\"%02d\", state_fips_rough))\n  ) %&gt;%\n  \n  # We don't want to include someone if they did not take the post-election survey\n  drop_na(commonpostweight)\n\nJust like last time, we should look at state samples. I’m not comfortable making an inference about a state if less than 200 people were polled there. Let’s find which states we should cull from our analysis.\n\n# Check sample sizes of each state\nsample_cutoff &lt;- ces2020_substantive %&gt;%\n  group_by(state_fips) %&gt;%\n  summarise(n = n()) %&gt;%\n  \n  # only give me states with less than 200 responses\n  filter(n &lt; 200)\n\nsample_cutoff\n\n# A tibble: 8 × 2\n  state_fips     n\n  &lt;chr&gt;      &lt;int&gt;\n1 02            89\n2 11           154\n3 15           178\n4 38           138\n5 44           173\n6 46           147\n7 50           134\n8 56            86\n\n\nThis means we are going to have to exclude states with fips code 56, 02, 50, 38, 46, 11, 44, and 15. Looking at the CES guide this is Wyoming, Alaska, Vermont, North Dakota, South Dakota, the District of Columbia (which is not a state right now!), Rhode Island, and Hawaii, respectively. Sad to see them go, but it’s better than making unsound claims!\nNow to calculate some statistics. Since I went through the survey package in more detail in my previous tutorial, I won’t reiterate myself here.\n\nsurvey &lt;- svydesign(ids = ~0, \n                    data = ces2020_substantive, \n                    weights = ~commonpostweight)\n\ncontact_state &lt;- as.data.frame(svyby(~contact, \n                                     ~state_fips, \n                                     survey, \n                                     svymean, \n                                     na.rm = TRUE)) %&gt;%\n  \n  # I'm telling R to select all state_fips that were not in our\n  # table of states with less than &lt;200 entries!\n  filter(!(state_fips %in% sample_cutoff$state_fips))\n\n# Join with urbnmapr\ncontact_map &lt;- left_join(contact_state, \n                         states, by = \"state_fips\") %&gt;%\n  \n  \n  # Spoiler! This step will let us make a tile map later on. We're specifying the dimensions of the \n  # tile here. I want my tiles to be square, but you can change this as you see fit.\n  mutate(xdimension = 1, \n         ydimension = 1) \n\nNow to plot! Let’s first try to map a standard U.S. state geography with a sequential scale (low to high, with no midpoint to facilitate diverging). There will be less comments this time around!\n\nggplot() +\n  # County map\n  geom_polygon(data = contact_map,\n               mapping = aes(x = long, y = lat,\n                             group = group,\n                             \n                             # our variable of interest!\n                             fill = contact)) +\n  # Add state outlines\n  geom_polygon(data = urbnmapr::states,\n               mapping = aes(long, lat,group = group),\n               \n               # with the color argument, I am giving\n               # states a grey outline!\n               fill = NA, color = \"grey30\", size = 0.4) +\n  # Projection\n  coord_map(projection = \"polyconic\")+\n  \n  # purple seems like a bipartisan color?\n  scale_fill_gradient(low = \"plum\", \n                      high = \"mediumpurple4\", \n                      labels = percent_format(accuracy = 5L),\n                      limits = c(0.35, 0.75),\n                      \n                      # This blank \"\" means I do not want my \n                      # legend to have a title.\n                      \"\") + \n  \n  # Theming\n  theme_minimal()+\n  theme(\n    legend.position = \"right\",\n    legend.text.align = 0,\n    plot.margin = unit(c(.5,.5,.2,.5), \"cm\")) +\n  theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n  ) + \n  theme(plot.title=element_text(family=\"Open Sans Condensed Bold\", margin=margin(b=15)))+\n  theme(plot.subtitle=element_text(family=\"Open Sans Condensed Light Italic\"))+\n  theme(plot.margin=unit(rep(0.5, 4), \"cm\"))+\n  labs(x = \"\",\n       y = \"\",\n       title = \"% of Respondents Contacted by a Political Campaign in 2020\",\n       caption = \"Author: Pia Deshpande, Data: 2020 Cooperative Election Study\")\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Open Sans Condensed Bold\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Open Sans Condensed Bold\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Open Sans Condensed Bold\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Open Sans Condensed Bold\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Open Sans Condensed Bold\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Open Sans Condensed Bold\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Open Sans Condensed Bold\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Open Sans Condensed Bold\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Open Sans Condensed Bold\"\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Open Sans Condensed Bold\"\n\n\n\n\n\n\n\n\n\nThere she is! The states that are missing are in stark white (notice how I did not have my scale start with white?). We see some states like Montana with a pretty high campaign contact rate, and soms states like Louisiana with a pretty low one. However, there’s a problem — we used the survey package to calculate these values, and they have standard errors. There’s not a good way to represent standard errors in our map as it currently exists!\nWhy are we worried about standard errors here? I was suspiciously silent about them with our first map. It’s because our first map was depicting something about CES respondents, which means our sample was the same as our population of interest (an incredible and rare thing). But when we analyze substantive questions, we’re usually trying to use the survey as a proxy for how a certain population behaves (in this case, the American public), which means our estimates have uncertainty to them. People who make maps and use them in their analysis are aware of this difficulty, and there are ways to overcome it that I won’t go into in this tutorial— mostly because I am not equipped to teach you something I do not know! Penn State’s Department of Geography has a good writeup on the topic of mapping uncertainty.\nBut for our tutorial, there is one problem I might try and solve — state geography. Shy of redrawing all the state lines, I think we should make a tile map. In a tile map, all states are represented by similar sized squares, so readers will weigh them equally as they visually process them.\n\n# The following code is adapted from the Urban Institute's Tutorial\n# https://urbaninstitute.github.io/r-at-urban/mapping.html#geom_tile()\n\n# create a custom geofacet grid\n# This was constructed by the Urban Institute! I am using it here.\n# It tells R how to draw the grid, and how to name each square.\n# I am grateful someone else wrote this and not me!\n\nurban_grid &lt;- tibble(\n  row = c(1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, \n          4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, \n          7, 7, 8, 8, 8),\n  col = c(1, 11, 6, 10, 11, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 4, 5, 6, 7, 8, 1, 4, 9),\n  code = c(\"AK\", \"ME\", \"WI\", \"VT\", \"NH\", \"WA\", \"ID\", \"MT\", \"ND\", \"MN\", \"IL\", \"MI\", \"NY\", \"MA\", \"OR\", \"NV\", \"WY\", \"SD\", \"IA\", \"IN\", \"OH\", \"PA\", \"NJ\", \"CT\", \"RI\", \"CA\", \"UT\", \"CO\", \"NE\", \"MO\", \"KY\", \"WV\", \"VA\", \"MD\", \"DE\", \"AZ\", \"NM\", \"KS\", \"AR\", \"TN\", \"NC\", \"SC\", \"DC\", \"OK\", \"LA\", \"MS\", \"AL\", \"GA\", \"HI\", \"TX\", \"FL\"),\n  name = c(\"Alaska\", \"Maine\", \"Wisconsin\", \"Vermont\", \"New Hampshire\", \"Washington\", \"Idaho\", \"Montana\", \"North Dakota\", \"Minnesota\", \"Illinois\", \"Michigan\", \"New York\", \"Massachusetts\", \"Oregon\", \"Nevada\", \"Wyoming\", \"South Dakota\", \"Iowa\", \"Indiana\", \"Ohio\", \"Pennsylvania\", \"New Jersey\", \"Connecticut\", \"Rhode Island\", \"California\", \"Utah\", \"Colorado\", \"Nebraska\", \"Missouri\", \"Kentucky\", \"West Virginia\", \"Virginia\", \"Maryland\", \"Delaware\", \"Arizona\", \"New Mexico\", \"Kansas\", \"Arkansas\", \"Tennessee\", \" North Carolina\", \"South Carolina\", \" District of Columbia\", \"Oklahoma\", \"Louisiana\", \"Mississippi\", \"Alabama\", \"Georgia\", \"Hawaii\", \"Texas\", \"Florida\")\n)\n\n\ncontact_map %&gt;%\n  \n  # remember when I defined xdimension and ydimension a while ago?\n  # It's coming in handy here!\n  ggplot(aes(x = xdimension, y = ydimension, fill = contact)) +\n\n  # We're making a tile map!\n  geom_tile() +\n  \n  # I am defining some display text here. I want\n  # to print the value of \"contact,\" which is the \n  # estimated percentage of Americans in a certain\n  # state who were contacted by a political campaign!\n  \n  # However, this number is a decimal. Another way\n  # of formatting percentages is by multiplying them by 100\n  # and using the round function (I wanted no decimal places).\n  # I then use the paste0 function to add a pretty percentage sign!\n  geom_text(aes(label = paste0(round(contact*100,0), \"%\")),\n            \n            # this makes the text white!\n            color = \"white\") +\n\n  # Using our grid from before. Thanks again to the Urban Institute!\n  # I also want to facet by state_abbv (That is, make a new tile) for\n  # each state abbreviation.\n  facet_geo(facets = ~state_abbv, grid = urban_grid) +\n  labs(title = \"Percentage of Americans Contacted by a Political Campaign in 2020\",\n       subtitle = \"Adapted from Code from the Urban Institute\",\n       caption = \"Graph by Pia Deshpande \\n Data from the 2020 CES\",\n       x = NULL,\n       y = NULL) +\n  scale_fill_gradient(\n    \n  # Purple all the way down!\n  low = \"plum\",\n  high = \"mediumpurple4\",\n  \n  # No legend title please\n  \"\"\n)+\n  theme(plot.background = element_rect(colour = \"white\"),\n        panel.grid = element_blank(),\n        panel.grid.major = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank(),\n        panel.spacing = unit(0L, \"pt\"),\n        legend.position = \"none\",\n        strip.text.x = element_text(size = 9L))\n\nNote: You provided a user-specified grid. If this is a generally-useful\n  grid, please consider submitting it to become a part of the geofacet\n  package. You can do this easily by calling:\n  grid_submit(__grid_df_name__)\n\n\n\n\n\n\n\n\n\n Okay! We have it! A graph-table-color thing! An abomination of nature that does fix some of our problems, but adds new ones. Readers are no longer going to see Texas as more important than Connecticut because of state size, and they can now look at the exact percentage. Data we excluded is also easy to spot! However, because this chart provides more information, it asks viewers to spend more time looking at it. This is a complicated figure—and you could argue that the color fill behind the text label doesn’t do much.\nAll maps are trade offs. In fact, even using a map is a pretty important choice! How should you decide?",
    "crumbs": [
      "tutorials",
      "Making maps with the CES"
    ]
  },
  {
    "objectID": "tutorial3.html#okay-when-should-i-use-a-map",
    "href": "tutorial3.html#okay-when-should-i-use-a-map",
    "title": "Making maps with the CES",
    "section": "Okay, when should I use a map?",
    "text": "Okay, when should I use a map?\n\nI am genuinely interested in answering a geographic question, and a map would help.\nI have thought carefully about the geography I am using and whether it is appropriate. (For example, if you are interested in studying different levels of property tax, the state geography will be too broad for you. Most property taxes are decided at the local level).\nI have thought about the trade offs of using maps and selected the best type of map. Tile maps are great when you don’t want the size of states, countries, or territories to make readers weigh larger geographic regions more importantly than small ones. On the other hand, state geography is recognizable, and can help people interpret your results. You have to make some important choices!\nI have thought about mapping uncertainty and am either comfortable with not doing it (see our first slew of maps) or have determined how I will signal uncertainty to readers.\n\nThat brings us to the end of our tutorial. As always, the whole script is below (slightly rearranged so it will hopefully run smoothly on your computer).",
    "crumbs": [
      "tutorials",
      "Making maps with the CES"
    ]
  },
  {
    "objectID": "tutorial3.html#the-whole-script",
    "href": "tutorial3.html#the-whole-script",
    "title": "Making maps with the CES",
    "section": "The whole script",
    "text": "The whole script\n\nknitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)\n# Setting up my RmD file\n\nlibrary(knitr)\n# A package for knitting things together!\nlibrary(htmlwidgets)\n# A package to save leaflet HTML so we can render these maps in Jekyll. If you are just looking at your maps on your local PC, you won't need to do this.\n# A special thanks to Rob Williams for his tutorial on how to do this: https://jayrobwilliams.com/posts/2020/09/jekyll-html\n\n#### LOAD PACKAGES ####\nlibrary(dataverse) # loading data\nlibrary(urbnmapr)  # geographic information for mapping!\nlibrary(ggplot2)   # pretty plots\nlibrary(geofacet)  # tile plots\nlibrary(scales)    # easy formatting\nlibrary(dplyr)     # data manipulation\nlibrary(survey)    # survey analysis\nlibrary(tidyverse) # the holy grail: more data wrangling\n\nlibrary(leaflet)   # optional: for interactive maps!\nlibrary(here)\n\nrequire(\"devtools\")\ndevtools::install_github(\"UrbanInstitute/urbnmapr\")\n##### LOADING DATA ####\nces2020_dataverse &lt;- get_dataframe_by_name(\n  filename = \"CES20_Common_OUTPUT_vv.dta\",\n  dataset = \"10.7910/DVN/E9N6PH\",\n  original = TRUE,\n  .f = haven::read_dta,\n  server = \"dataverse.harvard.edu\"\n)\n# Let's select variables that we care about!\nces2020_selected &lt;- ces2020_dataverse %&gt;%\n  select(\n    \n    # id for each respondent\n    caseid, \n    \n    # did you take the post-election survey?\n    tookpost, \n    \n    # state respondent is living in.\n    # In this code, I am both selecting\n    # the inputstate column and renaming\n    # it to state_fips rough, all in\n    # one line!\n    state_fips_rough = inputstate) %&gt;%\n  mutate(\n    \n    # tookpost is originally coded so 2 = Yes\n    # and 1 = No. I am recoding it so Yes = 1\n    # and No = 0. Why? When I do calculations, \n    # the average of tookpost will be a proportion\n    # between 0 and 1 that I can represent as a \n    # percent (ex. \"85% of respondents took the\n    # post-election survey\").\n    tookpost_recoded = case_when(\n      tookpost == 1 ~ 0,\n      tookpost == 2 ~ 1,\n    ),\n    \n    # why am I changing state_fips_rough? Like\n    # most coding, things were breaking which inspired\n    # me to get creative. Try running this chunk without\n    # this line and look at ces2020_selected. See anything\n    # weird? Check before you read on!\n    state_fips = as.character(sprintf(\"%02d\", state_fips_rough))\n    \n    # Okay, now that you've looked, I'll spill the beans. \n    # State FIPS codes are always two digits, like 15, 50, \n    # and 02. R was reading in single digits like 02 as\n    # just 2, which is a problem later on. To fix this\n    # I used the function sprintf to tell R to add\n    # leading zeroes to state_fips_rough until there\n    # were two digits per entry. So a value of 4 would\n    # turn into 04, but 36 would stay 36! I can't spend\n    # more time on this now, but if people are interested\n    # in this kind of data manipulation, I am happy to write\n    # about it!\n  )\n\n# hey R, for the following, use ces2020_selected please!\nces2020_selected %&gt;%\n  \n  # whatever calculation I tell you to do, please\n  # do it grouped--- that is, only within entries\n  # with the same state FIPS code.\n  group_by(state_fips) %&gt;%\n  \n  # could you tell me how many entries I have per\n  # state?\n  summarise(n = n())\n\n\n# you'll notice that the whole table does not print here.\n# To see the entire thing, print it in your R console\npost_aggregate &lt;- ces2020_selected %&gt;%\n  group_by(state_fips) %&gt;%\n  \n  # give me the mean of tookpost_recoded \n  # for each state, and make that a new\n  # variable called mean_post\n  summarise(mean_post = mean(tookpost_recoded))\n\n# let's take a look!\npost_aggregate\npost_map &lt;- left_join(post_aggregate, \n                      states, by = \"state_fips\")\n\npost_map\n# Plotting \n# Code adapted from: \n# https://www.cgoodman.com/blog/archives/2018/06/16/maps-in-r-using-urbnmapr/\n# I also retain some of Chris' annotations here for clarity.\n\nggplot() +\n  \n  # county map\n  geom_polygon(data = post_map,\n               mapping = aes(x = long, y = lat,\n                             group = group,\n                             \n                             # I want the color \n                             # that each state\n                             # has to depend\n                             # on mean_post!\n                             fill = mean_post)) +\n \n   # add state outlines using urbnmapr\n  geom_polygon(data = urbnmapr::states,\n               # when we write urbnmapr::states\n               # we are telling R to specifically\n               # use the urbnmapr package to pull up\n               # states. Some coders like using this notation\n               # consistently ---whenever they use a function\n               # they tell R which package it comes from.\n               # I don't do that, but if your code is\n               # breaking and you don't know why--- try\n               # calling on the package explicitly (sometimes)\n               # packages have functions that are named the\n               # same thing, which confuses R!\n               \n               mapping = aes(long, lat,group = group),\n               fill = NA, color = \"#ffffff\", size = 0.4) +\n  \n  # projection\n  coord_map(projection = \"polyconic\")+\n  \n  # I want to create a scale that goes from low to\n  # high, where the color of the high value connotes\n  # that things are good. In the United States,\n  # the color green is often the \"good\" or \"everything's\n  # fine\" color, so I'll use a version of that.\n  \n  # You can find an R Color Cheat Sheet here by Melanie Frazier: \n  # https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf\n  scale_fill_gradient(low = \"white\", \n                      high = \"seagreen4\", \n                      \n                      # I can define limits explicitly if I want.\n                      # If you do this, always check that you are not\n                      # accidentally excluding data! (I made sure all my)\n                      # data fit in this range reasonably.\n                      limits = c(0.75, 0.95), \n                      \n                      # Right now my code is in decimals.\n                      # We can use a trick we learned last tutorial\n                      # to make the labels into integer percentages.\n                      # Try changing the accuracy = 5L argument\n                      # to learn what it does!\n                      labels = percent_format(accuracy = 5L)) + \n  # Theming\n  theme_minimal()+\n  theme(\n    legend.position = \"right\",\n    legend.text.align = 0,\n    legend.title = element_blank(),\n    plot.margin = unit(c(.5,.5,.2,.5), \"cm\")) +\n    theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n  ) +\n  \n  # some useful titles!\n  labs(x = \"\",\n       y = \"\",\n       title = \"% of Respondents Who Took the Post Election Survey\",\n       caption = \"Author: Pia Deshpande, \\n Data: 2020 Cooperative Election Study\")\n\n# Plotting (Code adapted from https://www.cgoodman.com/blog/archives/2018/06/16/maps-in-r-using-urbnmapr/)\n\nggplot() +\n  # County map\n  geom_polygon(data = post_map,\n               mapping = aes(x = long, y = lat,\n                             group = group,\n                             fill = mean_post)) +\n  # Add state outlines\n  geom_polygon(data = urbnmapr::states,\n               mapping = aes(long, lat,group = group),\n               fill = NA, color = \"#ffffff\", size = 0.4) +\n  # Projection\n  coord_map(projection = \"polyconic\")+\n  \n# scale_fill_gradient 2 helps us create *diverging* color scales!\n# I define red as a low point, white as the middle, and green\n# as the high point. \nscale_fill_gradient2(\n  low = \"red\",\n  mid = \"white\",\n  high = \"seagreen4\",\n  \n  # I define the midpoint explicitly here! \n  # I calculate it not as the mean of all the \n  # state averages, but as the mean of all CES\n  # respondents. That value will be where the\n  # scale is a stark white.\n  midpoint = mean(ces2020_selected$tookpost_recoded),\n  limits = c(0.75, 0.95), \n  labels = percent_format(accuracy = 5L)\n)+\n  # Theming\n  theme_minimal()+\n  theme(\n    legend.position = \"right\",\n    legend.text.align = 0,\n    legend.title = element_blank(),\n    plot.margin = unit(c(.5,.5,.2,.5), \"cm\")) +\n    theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n  ) + \n  labs(x = \"\",\n       y = \"\",\n       title = \"% of Respondents Who Took the Post Election Survey\",\n       caption = \"Author: Pia Deshpande, \\n Data: 2020 Cooperative Election Study\")\n\n# Let's try making a substantive map: what percentage of respondents were contacted\n# by a political campaign in 2020?\n\n# Let's select variables that we care about!\nces2020_substantive &lt;- ces2020_dataverse %&gt;%\n  select(\n    \n    # id for each respondent\n    caseid, \n    \n    # weight for survey analysis!\n    commonpostweight, \n    \n    # state respondent is registered in\n    state_fips_rough = inputstate_post, \n    \n    # were you contacted by a political campaign in 2020?\n    CC20_431a) %&gt;%\n  \n  mutate(\n    contact = case_when(\n      \n      # I am recoding this so a \"Yes\" is a 1\n      # and a \"No\" is a 0.\n      CC20_431a == 1 ~ 1,\n      CC20_431a == 2 ~ 0\n    ),\n    state_fips = as.character(sprintf(\"%02d\", state_fips_rough))\n  ) %&gt;%\n  \n  # We don't want to include someone if they did not take the post-election survey\n  drop_na(commonpostweight)\n\n\n# Check sample sizes of each state\nsample_cutoff &lt;- ces2020_substantive %&gt;%\n  group_by(state_fips) %&gt;%\n  summarise(n = n()) %&gt;%\n  \n  # only give me states with less than 200 responses\n  filter(n &lt; 200)\n\nsample_cutoff\n\nsurvey &lt;- svydesign(ids = ~0, \n                    data = ces2020_substantive, \n                    weights = ~commonpostweight)\n\ncontact_state &lt;- as.data.frame(svyby(~contact, \n                                     ~state_fips, \n                                     survey, \n                                     svymean, \n                                     na.rm = TRUE)) %&gt;%\n  \n  # I'm telling R to select all state_fips that were not in our\n  # table of states with less than &lt;200 entries!\n  filter(!(state_fips %in% sample_cutoff$state_fips))\n\n# Join with urbnmapr\ncontact_map &lt;- left_join(contact_state, \n                         states, by = \"state_fips\") %&gt;%\n  \n  \n  # Spoiler! This step will let us make a tile map later on. We're specifying the dimensions of the \n  # tile here. I want my tiles to be square, but you can change this as you see fit.\n  mutate(xdimension = 1, \n         ydimension = 1) \nggplot() +\n  # County map\n  geom_polygon(data = contact_map,\n               mapping = aes(x = long, y = lat,\n                             group = group,\n                             \n                             # our variable of interest!\n                             fill = contact)) +\n  # Add state outlines\n  geom_polygon(data = urbnmapr::states,\n               mapping = aes(long, lat,group = group),\n               \n               # with the color argument, I am giving\n               # states a grey outline!\n               fill = NA, color = \"grey30\", size = 0.4) +\n  # Projection\n  coord_map(projection = \"polyconic\")+\n  \n  # purple seems like a bipartisan color?\n  scale_fill_gradient(low = \"plum\", \n                      high = \"mediumpurple4\", \n                      labels = percent_format(accuracy = 5L),\n                      limits = c(0.35, 0.75),\n                      \n                      # This blank \"\" means I do not want my \n                      # legend to have a title.\n                      \"\") + \n  \n  # Theming\n  theme_minimal()+\n  theme(\n    legend.position = \"right\",\n    legend.text.align = 0,\n    plot.margin = unit(c(.5,.5,.2,.5), \"cm\")) +\n  theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n  ) + \n  theme(plot.title=element_text(family=\"Open Sans Condensed Bold\", margin=margin(b=15)))+\n  theme(plot.subtitle=element_text(family=\"Open Sans Condensed Light Italic\"))+\n  theme(plot.margin=unit(rep(0.5, 4), \"cm\"))+\n  labs(x = \"\",\n       y = \"\",\n       title = \"% of Respondents Contacted by a Political Campaign in 2020\",\n       caption = \"Author: Pia Deshpande, Data: 2020 Cooperative Election Study\")\n# The following code is adapted from the Urban Institute's Tutorial\n# https://urbaninstitute.github.io/r-at-urban/mapping.html#geom_tile()\n\n# create a custom geofacet grid\n# This was constructed by the Urban Institute! I am using it here.\n# It tells R how to draw the grid, and how to name each square.\n# I am grateful someone else wrote this and not me!\n\nurban_grid &lt;- tibble(\n  row = c(1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, \n          4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, \n          7, 7, 8, 8, 8),\n  col = c(1, 11, 6, 10, 11, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 4, 5, 6, 7, 8, 1, 4, 9),\n  code = c(\"AK\", \"ME\", \"WI\", \"VT\", \"NH\", \"WA\", \"ID\", \"MT\", \"ND\", \"MN\", \"IL\", \"MI\", \"NY\", \"MA\", \"OR\", \"NV\", \"WY\", \"SD\", \"IA\", \"IN\", \"OH\", \"PA\", \"NJ\", \"CT\", \"RI\", \"CA\", \"UT\", \"CO\", \"NE\", \"MO\", \"KY\", \"WV\", \"VA\", \"MD\", \"DE\", \"AZ\", \"NM\", \"KS\", \"AR\", \"TN\", \"NC\", \"SC\", \"DC\", \"OK\", \"LA\", \"MS\", \"AL\", \"GA\", \"HI\", \"TX\", \"FL\"),\n  name = c(\"Alaska\", \"Maine\", \"Wisconsin\", \"Vermont\", \"New Hampshire\", \"Washington\", \"Idaho\", \"Montana\", \"North Dakota\", \"Minnesota\", \"Illinois\", \"Michigan\", \"New York\", \"Massachusetts\", \"Oregon\", \"Nevada\", \"Wyoming\", \"South Dakota\", \"Iowa\", \"Indiana\", \"Ohio\", \"Pennsylvania\", \"New Jersey\", \"Connecticut\", \"Rhode Island\", \"California\", \"Utah\", \"Colorado\", \"Nebraska\", \"Missouri\", \"Kentucky\", \"West Virginia\", \"Virginia\", \"Maryland\", \"Delaware\", \"Arizona\", \"New Mexico\", \"Kansas\", \"Arkansas\", \"Tennessee\", \" North Carolina\", \"South Carolina\", \" District of Columbia\", \"Oklahoma\", \"Louisiana\", \"Mississippi\", \"Alabama\", \"Georgia\", \"Hawaii\", \"Texas\", \"Florida\")\n)\n\n\ncontact_map %&gt;%\n  \n  # remember when I defined xdimension and ydimension a while ago?\n  # It's coming in handy here!\n  ggplot(aes(x = xdimension, y = ydimension, fill = contact)) +\n\n  # We're making a tile map!\n  geom_tile() +\n  \n  # I am defining some display text here. I want\n  # to print the value of \"contact,\" which is the \n  # estimated percentage of Americans in a certain\n  # state who were contacted by a political campaign!\n  \n  # However, this number is a decimal. Another way\n  # of formatting percentages is by multiplying them by 100\n  # and using the round function (I wanted no decimal places).\n  # I then use the paste0 function to add a pretty percentage sign!\n  geom_text(aes(label = paste0(round(contact*100,0), \"%\")),\n            \n            # this makes the text white!\n            color = \"white\") +\n\n  # Using our grid from before. Thanks again to the Urban Institute!\n  # I also want to facet by state_abbv (That is, make a new tile) for\n  # each state abbreviation.\n  facet_geo(facets = ~state_abbv, grid = urban_grid) +\n  labs(title = \"Percentage of Americans Contacted by a Political Campaign in 2020\",\n       subtitle = \"Adapted from Code from the Urban Institute\",\n       caption = \"Graph by Pia Deshpande \\n Data from the 2020 CES\",\n       x = NULL,\n       y = NULL) +\n  scale_fill_gradient(\n    \n  # Purple all the way down!\n  low = \"plum\",\n  high = \"mediumpurple4\",\n  \n  # No legend title please\n  \"\"\n)+\n  theme(plot.background = element_rect(colour = \"white\"),\n        panel.grid = element_blank(),\n        panel.grid.major = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank(),\n        panel.spacing = unit(0L, \"pt\"),\n        legend.position = \"none\",\n        strip.text.x = element_text(size = 9L))",
    "crumbs": [
      "tutorials",
      "Making maps with the CES"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pia Deshpande",
    "section": "",
    "text": "Hi! I’m Pia Deshpande — a PhD Student in the Travers Department of Political Science at UC Berkeley. I have a particular interest in how social media shapes American politics.\nPrior to starting at Berkeley, I was a pre-doctoral researcher at the Cooperative Election Study (CES) and a research assistant at the MIT Election Lab. I also worked as a data journalist for the Associated Press during the 2020 election, and interned at CNN International in Hong Kong and POLITICO in D.C. In May of 2020, I earned a B.A. in Political Science from Columbia University. Though I am not a journalist anymore, I continue to write for non-academic venues as a researcher. Currently, I have work in Monkey Cage and Lawfare.\nWhen I was at the CES, I wrote tutorials for my students on how to use R. I wrote these tutorials before I even began graduate school, so I take only tenuous responsibility for the prose they contain. My sheepishness aside, I have heard from others that they have been helpful teaching tools, so I wanted to put them up. You can find them under tutorials .\nYou can contact me at pia_deshpande[at]berkeley[dot]edu."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "teaching",
    "section": "",
    "text": "Senior Honors Thesis Statistics and Methods Tutor, Fall 2024 and Spring 2025\n\n\nInstructors: Terri Bimes and Amy Gurowitz\nI supervise students’ independent honor theses, providing advice on design, analysis, and coding. Each week I have three hours of office hours. So far, I have supervised projects using qualitative interviews, text analysis, spatial analysis using geocoding, and survey analysis.\n\n\n\n\nIntro to American Politics, UC Berkeley Graduate Student Instructor, Spring 2024\n\n\nInstructor: Paul Pierson\nHeld two weekly sections for a total of three hours of instruction time with an additional two hours of office hours. Graded important assignments and taught students fundamental concepts in American politics. Topics included collective action problems, legislative entrenchment, the history of slavery in the United States, and information asymmetry among voters.\n\n\n\nsee section syllabus",
    "crumbs": [
      "teaching"
    ]
  },
  {
    "objectID": "teaching.html#university-of-california-berkeley",
    "href": "teaching.html#university-of-california-berkeley",
    "title": "teaching",
    "section": "",
    "text": "Senior Honors Thesis Statistics and Methods Tutor, Fall 2024 and Spring 2025\n\n\nInstructors: Terri Bimes and Amy Gurowitz\nI supervise students’ independent honor theses, providing advice on design, analysis, and coding. Each week I have three hours of office hours. So far, I have supervised projects using qualitative interviews, text analysis, spatial analysis using geocoding, and survey analysis.\n\n\n\n\nIntro to American Politics, UC Berkeley Graduate Student Instructor, Spring 2024\n\n\nInstructor: Paul Pierson\nHeld two weekly sections for a total of three hours of instruction time with an additional two hours of office hours. Graded important assignments and taught students fundamental concepts in American politics. Topics included collective action problems, legislative entrenchment, the history of slavery in the United States, and information asymmetry among voters.\n\n\n\nsee section syllabus",
    "crumbs": [
      "teaching"
    ]
  },
  {
    "objectID": "teaching.html#tufts-university",
    "href": "teaching.html#tufts-university",
    "title": "teaching",
    "section": "Tufts University",
    "text": "Tufts University\n\n\nPolitical Science Research Methods Teaching Assistant, Spring 2022\n\n\nInstructor: Brian Schaffner\nHeld office hours and review sessions to support students learning R and statistics for political science. Topics included descriptive statistics, regression, experimental design, types of statistical tests, and replication. Led one lecture on the use of RMarkdown, project coding structure, and basic inferential statistics in R.\n\n\n\n\nPublic Opinion Lab, Teaching Assistant, Fall 2021 and Spring 2022\n\n\nInstructor: Brian Schaffner\nSupervised student research, theses, and independent projects on the matter of public opinion. In the spring, led a collaborative lab project on asymmetric polarization and weaponization of the term “critical race theory.”",
    "crumbs": [
      "teaching"
    ]
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "research",
    "section": "",
    "text": "Deshpande, Pia, Scott Blatte, Yonatan Margalit, Carolina Olea Lezama, Brian F. Schaffner, Aadhya Shivakumar,and David Wingens (2024). Critical race theory and asymmetric mobilization. Political Behavior, 46(3), 1677-1699.\n\n\nTeaching Critical Race Theory (CRT) in schools quickly became a salient issue nationally and in local elections despite CRT’s origins as an academic theory. In this paper, we argue that elite asymmetries regarding the importance of CRT spillover to the electorate. We show that Republican legislators and conservative media’s use of the term “critical race theory” dwarfed that of Democratic legislators and liberal media, respectively. A spike in general interest in the term happened concurrently with this elite push. We then hypothesize that in part due to this asymmetry in exposure to the term “critical race theory” itself in elite messaging, CRT policy may have an asymmetric effect on political mobilization, favoring Republicans, who tend to oppose the teaching of CRT in schools. To test this hypothesis, we conduct a survey experiment and find that Republicans presented with a pro-CRT policy change are politically mobilized, while Democrats presented with an anti-CRT policy change are not. In particular, Republicans exposed to the pro-CRT policy reported a higher likelihood of voting, encouraging others to vote, and contacting their local politicians. Thus, the case of CRT helps to illustrate the conditions under which issues can asymmetrically mobilize citizens.\n\n\n\nread paper\n\n\n\nreplication code\n\n\n\nTo cite this paper using BibTeX, use the following:\n\n\n@article{deshpande2024critical,\n  title={Critical race theory and asymmetric mobilization},\n  author={Deshpande, Pia and Blatte, Scott and Margalit,\n  Yonatan and Lezama, Carolina Olea and Schaffner, \n  Brian F and Shivakumar, Aadhya and Wingens, David},\n  journal={Political Behavior},\n  volume={46},\n  number={3},\n  pages={1677--1699},\n  year={2024},\n  publisher={Springer}\n}\n\n\n\nI do not include any of my writing as a journalist here (or my brief explainers for the MIT Election Lab — which you can handily read here). You can find select publications listed in my cv. If you are searching for a specific article, rest assured that they are on the internet forever! I know it may be difficult to track some down though, so feel free to email me.\n\n\nDeshpande P, Cha J. The 2020 CES: Duplicate Respondents and Handling Asian and Hispanic Subsamples.The Cooperative Election Study; 2022.\n\n\nPia Deshpande and Jeremiah Cha report that 25% of the 2020 CES sample took the 2018 CES. They report best practices for conducting analyses of ethnic subgroups and produce tables documenting counts for specific Hispanic (Mexican, Puerto Rican, South American, Spanish, and U.S.-identified Hispanics) and Asian (Indian, Chinese, Filipino, U.S.-identified Asians) subgroups. The report also compares the 2020 CES sample of Asians and Hispanics to the 2019 5-year American Community Survey and notes where samples differ.\n\n\nread paper\n\n\n\n\nDeshpande, Pia. “Election Administration Challenges in Ohio.”Lawfare, 30 Sept. 2020, https://www.lawfaremedia.org/article/election-administration-challenges-ohio.\n\n\nread paper\n\n\n\n\nCao, Diana, et al. “Election Administration Challenges in Pennsylvania.”Lawfare, 9 Oct. 2020, https://www.lawfaremedia.org/article/election-administration-challenges-pennsylvania.\n\n\nread paper\n\n\n\n\n\n\n\n“Swipe to Learn More: Understanding How TikTok Affects Users’ Beliefs” with Aaron Pope\n\n\nTikTok has evolved from a platform primarily for entertainment to a significant source of political content, with over half of U.S. TikTok users reporting that they use it as a news source. However, there is limited research on how TikTok use shapes individuals’ political beliefs. In a randomized experiment, we examine the effects of a 3-week TikTok deactivation on users’ perceptions of issue importance, political knowledge, and self-reported well-being. For a subset of participants, we also collect TikTok watch history data, which allows us to measure baseline levels of political content consumption on the platform. And for the control group, we are able to observe the content they are exposed to during the experiment period. We hypothesize that deactivating TikTok will lead users to view different political issues as more important, depending on which issues were more prominent on the app during the deactivation period.",
    "crumbs": [
      "research"
    ]
  },
  {
    "objectID": "research.html#peer-reviewed-articles",
    "href": "research.html#peer-reviewed-articles",
    "title": "research",
    "section": "",
    "text": "Deshpande, Pia, Scott Blatte, Yonatan Margalit, Carolina Olea Lezama, Brian F. Schaffner, Aadhya Shivakumar,and David Wingens (2024). Critical race theory and asymmetric mobilization. Political Behavior, 46(3), 1677-1699.\n\n\nTeaching Critical Race Theory (CRT) in schools quickly became a salient issue nationally and in local elections despite CRT’s origins as an academic theory. In this paper, we argue that elite asymmetries regarding the importance of CRT spillover to the electorate. We show that Republican legislators and conservative media’s use of the term “critical race theory” dwarfed that of Democratic legislators and liberal media, respectively. A spike in general interest in the term happened concurrently with this elite push. We then hypothesize that in part due to this asymmetry in exposure to the term “critical race theory” itself in elite messaging, CRT policy may have an asymmetric effect on political mobilization, favoring Republicans, who tend to oppose the teaching of CRT in schools. To test this hypothesis, we conduct a survey experiment and find that Republicans presented with a pro-CRT policy change are politically mobilized, while Democrats presented with an anti-CRT policy change are not. In particular, Republicans exposed to the pro-CRT policy reported a higher likelihood of voting, encouraging others to vote, and contacting their local politicians. Thus, the case of CRT helps to illustrate the conditions under which issues can asymmetrically mobilize citizens.\n\n\n\nread paper\n\n\n\nreplication code\n\n\n\nTo cite this paper using BibTeX, use the following:\n\n\n@article{deshpande2024critical,\n  title={Critical race theory and asymmetric mobilization},\n  author={Deshpande, Pia and Blatte, Scott and Margalit,\n  Yonatan and Lezama, Carolina Olea and Schaffner, \n  Brian F and Shivakumar, Aadhya and Wingens, David},\n  journal={Political Behavior},\n  volume={46},\n  number={3},\n  pages={1677--1699},\n  year={2024},\n  publisher={Springer}\n}\n\n\n\nI do not include any of my writing as a journalist here (or my brief explainers for the MIT Election Lab — which you can handily read here). You can find select publications listed in my cv. If you are searching for a specific article, rest assured that they are on the internet forever! I know it may be difficult to track some down though, so feel free to email me.\n\n\nDeshpande P, Cha J. The 2020 CES: Duplicate Respondents and Handling Asian and Hispanic Subsamples.The Cooperative Election Study; 2022.\n\n\nPia Deshpande and Jeremiah Cha report that 25% of the 2020 CES sample took the 2018 CES. They report best practices for conducting analyses of ethnic subgroups and produce tables documenting counts for specific Hispanic (Mexican, Puerto Rican, South American, Spanish, and U.S.-identified Hispanics) and Asian (Indian, Chinese, Filipino, U.S.-identified Asians) subgroups. The report also compares the 2020 CES sample of Asians and Hispanics to the 2019 5-year American Community Survey and notes where samples differ.\n\n\nread paper\n\n\n\n\nDeshpande, Pia. “Election Administration Challenges in Ohio.”Lawfare, 30 Sept. 2020, https://www.lawfaremedia.org/article/election-administration-challenges-ohio.\n\n\nread paper\n\n\n\n\nCao, Diana, et al. “Election Administration Challenges in Pennsylvania.”Lawfare, 9 Oct. 2020, https://www.lawfaremedia.org/article/election-administration-challenges-pennsylvania.\n\n\nread paper\n\n\n\n\n\n\n\n“Swipe to Learn More: Understanding How TikTok Affects Users’ Beliefs” with Aaron Pope\n\n\nTikTok has evolved from a platform primarily for entertainment to a significant source of political content, with over half of U.S. TikTok users reporting that they use it as a news source. However, there is limited research on how TikTok use shapes individuals’ political beliefs. In a randomized experiment, we examine the effects of a 3-week TikTok deactivation on users’ perceptions of issue importance, political knowledge, and self-reported well-being. For a subset of participants, we also collect TikTok watch history data, which allows us to measure baseline levels of political content consumption on the platform. And for the control group, we are able to observe the content they are exposed to during the experiment period. We hypothesize that deactivating TikTok will lead users to view different political issues as more important, depending on which issues were more prominent on the app during the deactivation period.",
    "crumbs": [
      "research"
    ]
  },
  {
    "objectID": "research.html#policy-and-data-reports",
    "href": "research.html#policy-and-data-reports",
    "title": "research",
    "section": "",
    "text": "I do not include any of my writing as a journalist here (or my brief explainers for the MIT Election Lab — which you can handily read here). You can find select publications listed in my cv. If you are searching for a specific article, rest assured that they are on the internet forever! I know it may be difficult to track some down though, so feel free to email me.\n\n\nDeshpande P, Cha J. The 2020 CES: Duplicate Respondents and Handling Asian and Hispanic Subsamples.The Cooperative Election Study; 2022.\n\n\nPia Deshpande and Jeremiah Cha report that 25% of the 2020 CES sample took the 2018 CES. They report best practices for conducting analyses of ethnic subgroups and produce tables documenting counts for specific Hispanic (Mexican, Puerto Rican, South American, Spanish, and U.S.-identified Hispanics) and Asian (Indian, Chinese, Filipino, U.S.-identified Asians) subgroups. The report also compares the 2020 CES sample of Asians and Hispanics to the 2019 5-year American Community Survey and notes where samples differ.\n\n\nread paper\n\n\n\n\nDeshpande, Pia. “Election Administration Challenges in Ohio.”Lawfare, 30 Sept. 2020, https://www.lawfaremedia.org/article/election-administration-challenges-ohio.\n\n\nread paper\n\n\n\n\nCao, Diana, et al. “Election Administration Challenges in Pennsylvania.”Lawfare, 9 Oct. 2020, https://www.lawfaremedia.org/article/election-administration-challenges-pennsylvania.\n\n\nread paper",
    "crumbs": [
      "research"
    ]
  },
  {
    "objectID": "research.html#working-papers-and-works-in-progress",
    "href": "research.html#working-papers-and-works-in-progress",
    "title": "research",
    "section": "",
    "text": "“Swipe to Learn More: Understanding How TikTok Affects Users’ Beliefs” with Aaron Pope\n\n\nTikTok has evolved from a platform primarily for entertainment to a significant source of political content, with over half of U.S. TikTok users reporting that they use it as a news source. However, there is limited research on how TikTok use shapes individuals’ political beliefs. In a randomized experiment, we examine the effects of a 3-week TikTok deactivation on users’ perceptions of issue importance, political knowledge, and self-reported well-being. For a subset of participants, we also collect TikTok watch history data, which allows us to measure baseline levels of political content consumption on the platform. And for the control group, we are able to observe the content they are exposed to during the experiment period. We hypothesize that deactivating TikTok will lead users to view different political issues as more important, depending on which issues were more prominent on the app during the deactivation period.",
    "crumbs": [
      "research"
    ]
  },
  {
    "objectID": "tutorial2.html",
    "href": "tutorial2.html",
    "title": "Plotting trends over time with the CES",
    "section": "",
    "text": "Published: November 11, 2021",
    "crumbs": [
      "tutorials",
      "Plotting trends over time with the CES"
    ]
  },
  {
    "objectID": "tutorial2.html#what-youll-learn",
    "href": "tutorial2.html#what-youll-learn",
    "title": "Plotting trends over time with the CES",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nHow to use parts of the survey package (specifically svyby)\nHow to use the dataverse package to pull data\nHow to use ggplot2 formatting to create a line chart\n\nSounds like a lot, but by the end of this you’ll be able to make some pretty interesting graphs with good representations of uncertainty. Who knows what other cool things you’ll find in our data?\nIt is also important to note what you won’t learn in this tutorial. You won’t be walked through how to combine CES data from multiple years, and the more complex data transformations sometimes required to make comparisons accross time. This tutorial focuses on starting with combined data and getting a useful plot! Others focused on data manipulation are incoming — please let us know what you’d like to see!\nBut before we get ahead of ourselves — packages! You’ll need the following to execute my code, which you can install with install.packages(\"name\"). So, if you want to install the survey package, you would type install.packages(\"survey\"). You’ll then need to load them with library, like I do here:\n\n#### LOAD PACKAGES ####\nlibrary(haven)      # loading in DTA data\nlibrary(dplyr)      # data transformation and column mutations\nlibrary(tidyverse)  # drop_na function\nlibrary(dataverse)  # pull data from Dataverse\nlibrary(survey)     # use survey weights",
    "crumbs": [
      "tutorials",
      "Plotting trends over time with the CES"
    ]
  },
  {
    "objectID": "tutorial2.html#getting-the-data",
    "href": "tutorial2.html#getting-the-data",
    "title": "Plotting trends over time with the CES",
    "section": "Getting the data",
    "text": "Getting the data\nObtaining data is sometimes the toughest part of doing social science research. Luckily, the CES has you covered. If you’re using CES data, you can load in all relevant datasets with the dataverse package in R. There is also a cumulative CES dataset created by the Shiro Kuriwaki and cumulative dataset on policy preferences by Angelo Dagonel.\nFor this tutorial, we will be using a cumulative file on political participation that I created with CES data. It covers the years 2008 to 2020. Data files can often be large and unwieldly, so instead of downloading onto my computer, I pull the data from Dataverse.\n\n##### LOADING DATA ####\nces_participation &lt;- get_dataframe_by_name(\n  filename = \"ces_participation_with_2022.csv\",\n  original = TRUE,\n  dataset = \"10.7910/DVN/JUX8KA\",\n  .f = read_csv,\n  server = \"dataverse.harvard.edu\"\n)\n\nAll you need to pull data is the file’s name, its dataset identifier, the server you are pulling from, and instructions on how to pull the data. You can find all of this at the file’s Dataverse link. Here is the link for ces_participation project. Once you click on the link, click again on the data file you’re interested in pulling (projects often have more than one data file!). There, under the tab “Metadata,” you’ll find the information we used to fill in the above command. Here is the link to the specific file.\nNow, we’ve located the file. . .but how do we know what to put in the .f argument? That argument is asking us what package and function we want to use to pull in the data. Since we are pulling in a dta file, we use haven::read_dta. You may notice that the file actually has a .tab extension. This is because Dataverse stores large dta files as tab files, but the dataverse package recognizes its original format if you write original = TRUE, and will pull the dta file.",
    "crumbs": [
      "tutorials",
      "Plotting trends over time with the CES"
    ]
  },
  {
    "objectID": "tutorial2.html#using-survey-weights",
    "href": "tutorial2.html#using-survey-weights",
    "title": "Plotting trends over time with the CES",
    "section": "Using survey weights",
    "text": "Using survey weights\nWeights are extremely important to most survey analysis. Though weighting algorithms can be complex, the reason for weights is simple: the people who answer our survey can be, in sum, quite different from the general population we are interested in. To fix this, we use weighting. I won’t go into more detail here. For a more detailed overview of weighting options out there, see Andrew Mercer, Arnold Lau, and Courtney Kennedy’s writeup for Pew on the topic.\nDoing analysis over multiple years can sometimes make weighting difficult. This is a basic tutorial, so I will not be using any time-series adjustments on our data, and we will be using the weights available. For 2016 onwards, the CES has pre and post-election weights available, and we will use the post-election ones. For previous years, one weight (caseweight) is provided, and is used. All of these weights have been combined into one column I called weights for this project.\nTo do anything with our weights, we first want to make a survey object! We can do this using the svydesign function, specifying the dataframe name and the name of the weights column:\n\nsurvey &lt;- svydesign(ids = ~0, \n                    data = ces_participation, \n                    weights = ~weight)\n\nA survey object will do all the complicated math for us if we treat it correctly. For example, I’m interested in looking at what percentage of respondents attended a political meeting in the past year, so what I really want is a crosstab by pol_meet_recode and year.\n\n# Plot participation over time\npol_meet &lt;- as.data.frame(svyby(~pol_meet_recode, \n                                ~year, \n                                survey, \n                                svymean, na.rm = TRUE)) \n\nLet’s take a look at what we found.\n\n# Plot participation over time\npol_meet\n\n     year pol_meet_recode          se\n2008 2008      0.13610472 0.002397564\n2010 2010      0.14410290 0.002162191\n2012 2012      0.11841966 0.002170677\n2014 2014      0.11515314 0.002081936\n2016 2016      0.10642031 0.002050197\n2018 2018      0.12255463 0.002096260\n2020 2020      0.07589015 0.001676549\n2022 2022      0.08806098 0.001718586\n\n\nWell, it seems like there’s a notable decline in local political meeting attendance in 2020. That makes sense! We were all locked in (or, we likely should have been if we weren’t an essential worker). The other differences we see are kind of small, and I haven’t even thought about dealing with those standard errors yet . . .\nIt’s at this point that I usually make a graph. If you are that special kind of genius that can calculate 95% confidence intervals in your head, I applaud you. I am not you, and I hope you stay for the fun plot aesthetics if nothing else. But before making a graph, what if I’m not just interested in overall trends? I’d like to know how these trends look different for Republicans and Democrats. Have no fear! We can do that with the survey package too; we’ll just need to subset!\n\npol_meet_rep &lt;- as.data.frame(svyby(~pol_meet_recode, \n                                    ~year, \n                                    subset(survey, pid3 == 2), \n                                    svymean, na.rm = T)) %&gt;%\n  \n  # I use the mutate function from dplyr here to make \n  # a new column (party) and have it contain the value \n  # \"Republican.\" This way, I'll be able to plot \n  # the differences between the two groups.\n  mutate(party = \"Republican\")\n\npol_meet_dem &lt;- as.data.frame(svyby(~pol_meet_recode,\n                                    ~year, \n                                    subset(survey, pid3 == 1), \n                                    svymean, na.rm = T)) %&gt;%\n  mutate(party = \"Democrat\")\n\nI am not going to print the results of pol_meet_rep or pol_meet_dem here, but feel free to look for yourself! You can compare this to the graph we create.\nHere ends our love affair with the survey package (at least for this tutorial). For more on the package, please see this excellent tutorial by Zachary Hertz (a Tufts alum now getting his Master’s at UChicago). There is also an example of survey analysis written up by statistician and R wizard Thomas Lumley, who authored the survey package. For STATA users (who no doubt are feeling left out during my tutorial), see the CES’ very own Brian Schaffner’s posts on survey analysis.\nAstute readers may be confused by my decision to surround our weighted crosstabs with as.data.frame, and they have every right to be. If you just need to look at the numbers on their own, there is no need to change a crosstab’s format to a data frame, but we need to mess with the table it produces, and the data frame format is best for such chicanery. (Try taking away as.data.frame from the previous chunk of code and completing the tutorial that way — the error messages you see will likely be instructive).\nThe last thing we’ll need to do before plotting is combining our two dataframes — this way, information about Republicans and Democrats is in one easy place for ggplot2.\n\npol_meet_party &lt;- pol_meet_rep %&gt;% \n  bind_rows(pol_meet_dem)",
    "crumbs": [
      "tutorials",
      "Plotting trends over time with the CES"
    ]
  },
  {
    "objectID": "tutorial2.html#plotting-things",
    "href": "tutorial2.html#plotting-things",
    "title": "Plotting trends over time with the CES",
    "section": "Plotting things",
    "text": "Plotting things\nWe can start by writing a command like the one we have below. It tells the package what dataset we want it to use (pol_meet_party) and defines its aesthetic arguments: aes(x=year, y=pol_meet_recode, group = party, color = party).\n\nplot_party &lt;- ggplot(pol_meet_party, \n                     aes(x=year, \n                         y=pol_meet_recode, \n                         group = party, \n                         color = party)) \n\nDrum roll please!\n\nplot_party \n\n\n\n\n\n\n\n\nOkay. . . so not much happened. It turns out we need to tell ggplot what to put on the graph! Let’s try again, but this time tell it that we want points and lines connecting them.\n\nplot_party &lt;- ggplot(pol_meet_party, \n                     aes(x=year, \n                         y=pol_meet_recode, \n                         group = party, \n                         color = party)) + \n  geom_line() +\n  geom_point()\n\nplot_party\n\n\n\n\n\n\n\n\nAll right! So that’s better, but not exactly where we want to be. The numbers on the left are hard to read because they’re decimals and not percentages, and there is no good representation of uncertainty on this graph! We don’t want someone to read this without knowing that surveys are fallible, do we? Let’s try sprucing this up:\n\nplot_party &lt;- ggplot(pol_meet_party, \n                     aes(x=year, \n                         y=pol_meet_recode, \n                         group = party, \n                         color = party)) + \n  geom_line() +\n  geom_point()+\n  geom_errorbar(aes(ymin=pol_meet_recode-1.96*se, \n                    ymax=pol_meet_recode+1.96*se), \n                width=.2,\n                position=position_dodge(0.05)) \n\nplot_party\n\n\n\n\n\n\n\n\nThis is looking a lot better. We added 95% confidence intervals by multiplying the standard errors by 1.96 (the relevant Z-score) and adding and subtracting them from our point estimates.\nBut there are still some problems. . .let’s do some final changes! This time, I’ll annotate what I’m doing in my code chunk, and we’ll do more steps at once.\n\nplot_party &lt;- ggplot(pol_meet_party, \n                     aes(x=year, \n                         y=pol_meet_recode, \n                         group = party, \n                         color = party)) + \n  geom_line() +\n  geom_point()+\n  geom_errorbar(aes(ymin=pol_meet_recode-1.96*se, \n                    ymax=pol_meet_recode+1.96*se), \n                width=.2,\n                position=position_dodge(0.05)) + \n  theme_minimal() + \n    # This will get rid of that grey background\n  \n  theme(plot.title = element_text(hjust = 0.5), \n        plot.caption = element_text(hjust = 1,\n                                    face = \"italic\"),\n        legend.title = element_blank()) + \n    # This sets up some stylistic standards! \n    # It makes sure our title (if we have one)\n    # is justified and makes our caption italic. \n    # It also gets rid of the title for our legend \n    # (we don't need it--- people will recognize \n    # the party names).\n  \n  scale_y_continuous(labels = scales::percent_format(accuracy = 1L),\n                     limits = c(0,0.2)) + \n    # This fixes our y_axis problem! It will make\n    # our numbers percentages with percent_format, \n    # and I've used accuracy = 1L to get rid of decimal\n    # points--we don't want to make readers assume we \n    # can be more exact than is true.\n    #\n    # I also manually set y-axis limits with \n    # limits = c(0,0.2). Note that I can't write\n    # c(0,20), because our data is not in percentages\n    # (though this command has it display that way). \n    # Why do I change the y-axis limits? \n    # Sometimes zooming into a line chart can make\n    # small changes look massive.\n    \n  ylab(\"% attended a local political meeting in the past year\") + \n  # Y-axis title!\n  \n  labs(caption = \"Plot: Pia Deshpande \\nData: CES\") +\n  # A handy dandy citation caption! It'll be \n  # italicized because of what we did earlier. \n  # The \\n mandates a line break.\n  \n  scale_color_manual(values=c(\"blue\", \"red\")) +\n  # Republicans should probably be red \n  # and Democrats should probably be blue!\n  \n  ggtitle(\"\")\n  # You often don't need an additional title!\n  # But you can always fill one in if you want.\n\nplot_party\n\n\n\n\n\n\n\n\nAnd finally. . . here’s the result! It seems like partisans are most motivated to attend local meetings when the opposing party has presidential power. Of course, we don’t exactly know why that is yet. Maybe you’ll fnd out!\nIf you’re new to ggplot, this might be overwhelming. That is okay. There are plenty of resources to help you get started, and you’ll learn more techniques as you make more plots. I am certainly not a ggplot2 expert, so I’ll cite some you can refer to. Selva Prabhakaran has written a comprehensive tutorial on plotting with the package. The open source book R for Data Science has a chapter on Data visualization that uses ggplot. Here is the offical tidyverse page for ggplot2, which has even more resources! It’s likely you may have different aesthetic desires than I do, and that is completely okay. Go wild! I’m excited to see the plots you make.",
    "crumbs": [
      "tutorials",
      "Plotting trends over time with the CES"
    ]
  },
  {
    "objectID": "tutorial2.html#the-whole-script",
    "href": "tutorial2.html#the-whole-script",
    "title": "Plotting trends over time with the CES",
    "section": "The whole script",
    "text": "The whole script\nHere is the entirety of my script all in one place! It has a few more plots that you can look at. Try to mess with the ggplot formatting provided to see how it alters the graphs produced.\n\n#### LOAD PACKAGES ####\nlibrary(haven)      # loading in DTA data\nlibrary(dplyr)      # data transformation and column mutations\nlibrary(tidyverse)  # drop_na function\nlibrary(dataverse)  # pull data from Dataverse\nlibrary(survey)     # use survey weights\n\n\n##### LOADING DATA ####\n\nces_participation &lt;- get_dataframe_by_name(\n  filename = \"ces_participation_with_2022.csv\",\n  original = TRUE,\n  dataset = \"10.7910/DVN/JUX8KA\",\n  .f = read_csv,\n  server = \"dataverse.harvard.edu\"\n)\n\n\nsurvey &lt;- svydesign(ids = ~0, data = ces_participation, weights = ~weight)\n\n\n# pol_meet By Party\npol_meet &lt;- as.data.frame(svyby(~pol_meet_recode, \n                                ~year, \n                                survey, \n                                svymean, \n                                na.rm = TRUE))\n\npol_meet_rep &lt;- as.data.frame(svyby(~pol_meet_recode, \n                                    ~year, \n                                    subset(survey, pid3 == 2),\n                                    svymean, \n                                    na.rm = T)) %&gt;%\n  mutate(party = \"Republican\")\n\npol_meet_dem &lt;- as.data.frame(svyby(~pol_meet_recode, \n                                    ~year, \n                                    subset(survey, pid3 == 1), \n                                    svymean, \n                                    na.rm = T)) %&gt;%\n  mutate(party = \"Democrat\")\n\n\npol_meet_party &lt;- pol_meet_rep %&gt;% \n  bind_rows(pol_meet_dem) \n\nplot_party &lt;- ggplot(pol_meet_party, \n                     aes(x=year, \n                         y=pol_meet_recode, \n                         group = party, \n                         color = party)) + \n  geom_line() +\n  geom_point()+\n  geom_errorbar(aes(ymin=pol_meet_recode-1.96*se, \n                    ymax=pol_meet_recode+1.96*se), \n                width=.2,\n                position=position_dodge(0.05)) +  \n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), \n        plot.caption = element_text(hjust = 1,\n                                    face = \"italic\"),\n        legend.title = element_blank()) + \n  scale_y_continuous(labels = scales::percent_format(accuracy = 1L)) + \n  ylab(\"% attended a local political meeting in the past year\") + \n  labs(caption = \"Plot: Pia Deshpande \\nData: CES\") +\n  scale_color_manual(values=c(\"blue\", \"red\")) +\n  ggtitle(\"\")\n\n\n# donate_candidate by Party\ndonate_candidate &lt;- as.data.frame(svyby(~donate_candidate_recode, \n                                        ~year, \n                                        survey, \n                                        svymean, \n                                        na.rm = TRUE))\n\ndonate_candidate_rep &lt;- as.data.frame(svyby(~donate_candidate_recode, \n                                            ~year, \n                                            subset(survey, pid3 == 2), \n                                            svymean, na.rm = T)) %&gt;%\n  mutate(party = \"Republican\")\n\ndonate_candidate_dem &lt;- as.data.frame(svyby(~donate_candidate_recode, \n                                            ~year, \n                                            subset(survey, pid3 == 1), \n                                            svymean, \n                                            na.rm = T)) %&gt;%\n  mutate(party = \"Democrat\")\n\n\ndonate_candidate_party &lt;- donate_candidate_rep %&gt;% \n  bind_rows(donate_candidate_dem) \n\n\ndonate_party &lt;- ggplot(donate_candidate_party, \n                       aes(x=year, y=donate_candidate_recode, \n                           group = party, \n                           color = party)) + \n  geom_line() +\n  geom_point()+\n  geom_errorbar(aes(ymin=donate_candidate_recode-1.96*se, \n                    ymax=donate_candidate_recode+1.96*se), \n                width=.2,\n                position=position_dodge(0.05)) +  \n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), \n        plot.caption = element_text(hjust = 1,\n                                    face = \"italic\"),\n        legend.title = element_blank()) + \n  scale_y_continuous(labels = scales::percent_format(accuracy = 1L)) + \n  ylab(\"% donated to a candidate in the past year\") + \n  labs(caption = \"Plot: Pia Deshpande \\nData: CES\") +\n  scale_color_manual(values=c(\"blue\", \"red\")) +\n  ggtitle(\"\")\n\n\n# work_candidate by Party\nwork_candidate &lt;- as.data.frame(svyby(~work_candidate_recode, \n                                      ~year, \n                                      survey, \n                                      svymean, na.rm = TRUE))\n\nwork_candidate_rep &lt;- as.data.frame(svyby(~work_candidate_recode, \n                                          ~year, \n                                          subset(survey, pid3 == 2), \n                                          svymean, na.rm = T)) %&gt;%\n  mutate(party = \"Republican\")\n\nwork_candidate_dem &lt;- as.data.frame(svyby(~work_candidate_recode, \n                                          ~year, subset(survey, pid3 == 1), \n                                          svymean, na.rm = T)) %&gt;%\n  mutate(party = \"Democrat\")\n\n\nwork_candidate_party &lt;- work_candidate_rep %&gt;% \n  bind_rows(work_candidate_dem) \n\nwork_party &lt;- ggplot(work_candidate_party, aes(x=year, \n                                               y=work_candidate_recode, \n                                               group = party, \n                                               color = party)) + \n  geom_line() +\n  geom_point()+\n  geom_errorbar(aes(ymin=work_candidate_recode-1.96*se, \n                    ymax=work_candidate_recode+1.96*se), width=.2,\n                position=position_dodge(0.05)) +  # these colors are not plotting right now\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), \n        plot.caption = element_text(hjust = 1,\n                                    face = \"italic\"),\n        legend.title = element_blank()) + \n  scale_y_continuous(labels = scales::percent_format(accuracy = 1L))+ \n  ylab(\"% worked for a candidate in the past year\") + \n  labs(caption = \"Plot: Pia Deshpande \\nData: CES\") +\n  scale_color_manual(values=c(\"blue\", \"red\")) +\n  ggtitle(\"\")\n\n# put_sign by Party\nput_sign &lt;- as.data.frame(svyby(~put_sign_recode, \n                                ~year, \n                                survey, \n                                svymean, na.rm = TRUE))\n\nput_sign_rep &lt;- as.data.frame(svyby(~put_sign_recode, \n                                    ~year, \n                                    subset(survey, pid3 == 2), \n                                    svymean, na.rm = T)) %&gt;%\n  mutate(party = \"Republican\")\n\nput_sign_dem &lt;- as.data.frame(svyby(~put_sign_recode, \n                                    ~year, \n                                    subset(survey, pid3 == 1), \n                                    svymean, na.rm = T)) %&gt;%\n  mutate(party = \"Democrat\")\n\n\nput_sign_party &lt;- put_sign_rep %&gt;% \n  bind_rows(put_sign_dem) \n\nput_sign_party &lt;- ggplot(put_sign_party, \n                         aes(x=year, \n                             y=put_sign_recode, \n                             group = party, \n                             color = party)) + \n  geom_line() +\n  geom_point()+\n  geom_errorbar(aes(ymin=put_sign_recode-1.96*se, \n                    ymax=put_sign_recode+1.96*se), \n                width=.2,\n                position=position_dodge(0.05)) +  \n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), \n        plot.caption = element_text(hjust = 1,\n                                    face = \"italic\"),\n        legend.title = element_blank()) + \n  scale_y_continuous(labels = scales::percent_format(accuracy = 1L)) + \n  ylab(\"% put political sign up in the past year\") + \n  labs(caption = \"Plot: Pia Deshpande \\nData: CES\") +\n  scale_color_manual(values=c(\"blue\", \"red\")) +\n  ggtitle(\"\")",
    "crumbs": [
      "tutorials",
      "Plotting trends over time with the CES"
    ]
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "cv",
    "section": "",
    "text": "If you are having a hard time viewing my CV, you can download it here.",
    "crumbs": [
      "cv"
    ]
  }
]